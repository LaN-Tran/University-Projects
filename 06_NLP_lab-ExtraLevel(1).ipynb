{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"Introduction-to-Python-and-Natural-Language-Technologies\">Introduction to Python and Natural Language Technologies</h1>\n",
    "<h2 id=\"Laboratory-06,-NLP-Introduction\">Laboratory 06, NLP Introduction</h2>\n",
    "<p><strong>March 18, 2020</strong></p>\n",
    "<p><strong>&Aacute;d&aacute;m Kov&aacute;cs</strong></p>\n",
    "<p>During this laboratory we are going to use a classification dataset of SemEval 2019 - Task 6. This is called Identifying and Categorizing Offensive Language in Social Media.</p>\n",
    "<h2 id=\"Preparation\">Preparation</h2>\n",
    "<p style=\"padding-left: 40px;\"><a href=\"http://sandbox.hlt.bme.hu/~adaamko/glove.6B.100d.txt\" target=\"_blank\" rel=\"noopener\">Download GLOVE</a>(and place it into this directory)</p>\n",
    "<p style=\"padding-left: 40px;\">Download the dataset (with python code)</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('data/offenseval.tsv', <http.client.HTTPMessage at 0x7fe7f8183100>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "if not os.path.isdir('./data'):\n",
    "    os.mkdir('./data')\n",
    "\n",
    "import urllib\n",
    "u = urllib.request.URLopener()\n",
    "u.retrieve(\"http://sandbox.hlt.bme.hu/~adaamko/offenseval.tsv\", \"data/offenseval.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Train a Logistic Regression on the dataset\n",
    "\n",
    "Use a CountVectorizer for featurizing your data. You can reuse the code presented during the lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Read in the dataset into a Pandas DataFrame\n",
    "Use `pd.read_csv` with the correct parameters to read in the dataset. If done correctly, `DataFrame` should have 3 columns, \n",
    "`id`, `tweet`, `subtask_a`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "245ae3153d23ed632c3f49c1f0478d3e",
     "grade": false,
     "grade_id": "cell-28244ef9e290dace",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>86426</td>\n",
       "      <td>@USER She should ask a few native Americans wh...</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90194</td>\n",
       "      <td>@USER @USER Go home you’re drunk!!! @USER #MAG...</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16820</td>\n",
       "      <td>Amazon is investigating Chinese employees who ...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>62688</td>\n",
       "      <td>@USER Someone should'veTaken\" this piece of sh...</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>43605</td>\n",
       "      <td>@USER @USER Obama wanted liberals &amp;amp; illega...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13236</th>\n",
       "      <td>95338</td>\n",
       "      <td>@USER Sometimes I get strong vibes from people...</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13237</th>\n",
       "      <td>67210</td>\n",
       "      <td>Benidorm ✅  Creamfields ✅  Maga ✅   Not too sh...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13238</th>\n",
       "      <td>82921</td>\n",
       "      <td>@USER And why report this garbage.  We don't g...</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13239</th>\n",
       "      <td>27429</td>\n",
       "      <td>@USER Pussy</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13240</th>\n",
       "      <td>46552</td>\n",
       "      <td>#Spanishrevenge vs. #justice #HumanRights and ...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13240 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              tweet subtask_a\n",
       "1      86426  @USER She should ask a few native Americans wh...       OFF\n",
       "2      90194  @USER @USER Go home you’re drunk!!! @USER #MAG...       OFF\n",
       "3      16820  Amazon is investigating Chinese employees who ...       NOT\n",
       "4      62688  @USER Someone should'veTaken\" this piece of sh...       OFF\n",
       "5      43605  @USER @USER Obama wanted liberals &amp; illega...       NOT\n",
       "...      ...                                                ...       ...\n",
       "13236  95338  @USER Sometimes I get strong vibes from people...       OFF\n",
       "13237  67210  Benidorm ✅  Creamfields ✅  Maga ✅   Not too sh...       NOT\n",
       "13238  82921  @USER And why report this garbage.  We don't g...       OFF\n",
       "13239  27429                                        @USER Pussy       OFF\n",
       "13240  46552  #Spanishrevenge vs. #justice #HumanRights and ...       NOT\n",
       "\n",
       "[13240 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_dataset():\n",
    "    dataset = pd.read_csv(\"./data/offenseval.tsv\",sep=\"\\t\", names = [\"id\", \"tweet\", \"subtask_a\"]) # there is an unecessary row 0th\n",
    "    # how to determine the seperator in the file.tsv:\n",
    "    # first step:\n",
    "    # run: dataset = pd.read_csv(\"./data/offenseval.tsv\", names = [\"id\", \"tweet\", \"subtask_a\"])\n",
    "    # print out the 'dataset'\n",
    "    # in the column which containes the text, we can see the seperator by comparing it with the file.tsv in the directory where we loaded it from\n",
    "    \n",
    "    final_dataset= dataset.iloc[1:]\n",
    "    return final_dataset\n",
    "d=read_dataset()\n",
    "type(d)\n",
    "d\n",
    "    #raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dac5b55254d3e1bc940aabb8b98b3f11",
     "grade": true,
     "grade_id": "cell-8cdaaf32c6a9bcd4",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "train_data_unprocessed = read_dataset()\n",
    "\n",
    "assert type(train_data_unprocessed) == pd.core.frame.DataFrame\n",
    "assert len(train_data_unprocessed.columns) == 3\n",
    "assert (train_data_unprocessed.columns == ['id', 'tweet', 'subtask_a']).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Convert `subtask_a` into a binary label\n",
    "The task is to classify the given tweets into two category: _offensive(OFF)_ , _not offensive (NOT)_. For machine learning algorithms you will need integer labels instead of strings. Add a new column to the dataframe called `label`, and transform the `subtask_a` column into a binary integer label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f0a97018f8bd5fdc07b40efdb873d503",
     "grade": false,
     "grade_id": "cell-731e83e7c0331c22",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def transform(train_data):\n",
    "    # YOUR CODE HERE\n",
    "    train_data[\"label\"]=train_data.subtask_a.apply(lambda x: 1 if x== \"NOT\" else 0 )\n",
    "    # reference for 'if, else' in lambda : https://thispointer.com/python-how-to-use-if-else-elif-in-lambda-functions/ \n",
    "    return train_data\n",
    "    #raise NotImplementedError()\n",
    "    \n",
    "d=transform(train_data_unprocessed)\n",
    "d\n",
    "type(d.iloc[0, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eff0405739147a8bb4b0584bca759c50",
     "grade": true,
     "grade_id": "cell-9162edb7c3cbea87",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from pandas.api.types import is_numeric_dtype\n",
    "\n",
    "train_data = transform(train_data_unprocessed)\n",
    "\n",
    "assert \"label\" in train_data\n",
    "assert is_numeric_dtype(train_data.label)\n",
    "assert (train_data.label.isin([0,1])).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    4400\n",
       "1    8840\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.groupby(\"label\").size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Initialize CountVectorizer and _train_ it on the _tweet_ column of the dataset\n",
    "The _training_ will prepare the vocabulary for us so we will be able to use it for training a LogisticRegression algorithm later. Set the number of `max_features` to 5000 so vocabulary won't be too big for training. Also filter out english `stop_words`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dc3d77a0b35ffc7e7a1bf4117acf76a8",
     "grade": false,
     "grade_id": "cell-c14091eabbbdefb8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# We will need to use a random seed for our methods so they will be reproducible\n",
    "SEED = 1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e11dcb24d6bd42ab3f2bbc0d516b6a25",
     "grade": false,
     "grade_id": "cell-d66cb77a6cdd86cb",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# BAG OF WORDS:\n",
    "\n",
    "def prepare_vectorizer(train_data):\n",
    "    # YOUR CODE HERE\n",
    "    vectorizer = CountVectorizer(max_features=5000, stop_words=\"english\")\n",
    "    \n",
    "    X = vectorizer.fit(train_data.tweet) # create a bag of words for the nlp machine learning\n",
    "                                         # each word in the bag is a unique word in the whole text\n",
    "                                         # (in our case, the text is the composition of all english and distinctive words\n",
    "                                         #  in the tweet column of the train_data set ).\n",
    "                                         # each word is a feature.\n",
    "                                         # stop_words=\"english\" : used to exclude all the stop words which we get from the 'english' library out of our feature vector.   \n",
    "    return X\n",
    "\n",
    "    #raise NotImplementedError()\n",
    "vectorizer = prepare_vectorizer(train_data)\n",
    "vectorizer.vocabulary_\n",
    "vectorizer\n",
    "\n",
    "# [\"hello this is the intro to nlp\"] is an input text,\n",
    "# => we need to convert it to a vector with 5000 elements, each element represents 1 if the word in the input text corresponds\n",
    "# to a feature (a word) of our bag of words (which contains 5000....)\n",
    "transformed = vectorizer.transform([\"hello this is the intro to nlp\"])\n",
    "transformed\n",
    "transformed = vectorizer.transform([\"hello this is the intro to nlp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "652a63730eb7cb7cc7e0bfba7e726265",
     "grade": true,
     "grade_id": "cell-edef3ff41f4c1bde",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "vectorizer = prepare_vectorizer(train_data)\n",
    "\n",
    "transformed = vectorizer.transform([\"hello this is the intro to nlp\"])\n",
    "assert transformed.dtype == np.dtype('int64')\n",
    "assert transformed.shape == (1, 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Featurize the dataset with the prepared CountVectorizer, and split it into _train_ and _test_ dataset\n",
    "You should use the random seed when you are splitting the dataset. The scale of the training and the test dataset should be 70% to 30%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a55b9f4dcb26e14c42f9ab6e7822b2f9",
     "grade": false,
     "grade_id": "cell-2f4b7f4e32e2bc36",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split as split\n",
    "\n",
    "def vectorize_to_bow(tr_data, tst_data, vectorizer):   # bow: bag of words\n",
    "    # YOUR CODE HERE\n",
    "    tr_vectors = vectorizer.transform(tr_data) # transform the textdatas into numerical datas for the input of the machine learning (in our case, called nlp)\n",
    "    \n",
    "    tst_vectors = vectorizer.transform(tst_data)\n",
    "    return tr_vectors, tst_vectors\n",
    "    #raise NotImplementedError()\n",
    "\n",
    "def get_features_and_labels(data, labels, vectorizer):\n",
    "    \n",
    "    # tr_data,tst_data,tr_labels,tst_labels = split...\n",
    "    # ...\n",
    "    # tr_vecs, tst_vecs = vectorize_to_bow(...\n",
    "    # YOUR CODE HERE\n",
    "    tr_data,tst_data,tr_labels,tst_labels = split(data,labels, test_size=0.3, random_state=1234)\n",
    "    \n",
    "    tst_vecs = []\n",
    "    tr_vecs = []\n",
    "    tr_vecs, tst_vecs = vectorize_to_bow(tr_data, tst_data, vectorizer)    \n",
    "    return tr_vecs, tr_labels, tst_vecs, tst_labels\n",
    "    #raise NotImplementedError()\n",
    "tr_vecs, tr_labels, tst_vecs, tst_labels = get_features_and_labels(train_data.tweet, train_data.label, vectorizer)\n",
    "tr_vecs.shape\n",
    "type(tr_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ff6f2048306f4c58336f5e2b88065068",
     "grade": true,
     "grade_id": "cell-8cb9baeaccf044d7",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "tr_vecs, tr_labels, tst_vecs, tst_labels = get_features_and_labels(train_data.tweet, train_data.label, vectorizer)\n",
    "assert tr_vecs.shape == (9268, 5000)\n",
    "assert tr_labels.shape == (9268,)\n",
    "assert tst_vecs.shape == (3972, 5000)\n",
    "assert tst_labels.shape == (3972,)\n",
    "assert tr_vecs[0].toarray().shape == (1, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import a bunch of stuff from sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# We will train a LogisticRegression algorithm for the classification\n",
    "lr  = LogisticRegression(n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Train and evaluate your method!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0a225537e2b31495a027d806dcd7d6a8",
     "grade": false,
     "grade_id": "cell-58727c0fba0e7d13",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(n_jobs=-1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training on the train dataset\n",
    "# YOUR CODE HERE\n",
    "lr.fit(tr_vecs, tr_labels)\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "55ab78ccc35209c5f1716b20a88d7bc4",
     "grade": true,
     "grade_id": "cell-a6e0028e6cfeaa33",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.utils.validation import check_is_fitted\n",
    "from sklearn.exceptions import NotFittedError\n",
    "\n",
    "try:\n",
    "    check_is_fitted(lr)\n",
    "except NotFittedError as e:\n",
    "    assert None, repr(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "96755e97d86dd7ceb2261886a1f95430",
     "grade": false,
     "grade_id": "cell-eef0add921511581",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Evaluation on the test dataset\n",
    "def preds(lr, tst_vecs):\n",
    "    # YOUR CODE HERE\n",
    "    #print(type(tst_vecs))\n",
    "\n",
    "    lr_pred = lr.predict(tst_vecs)\n",
    "    #print(\"Logistic Regression Test accuracy : {}\".format(accuracy_score(tst_labels, lr_pred)))\n",
    "    return lr_pred\n",
    "    #raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f91da323e048cc77e1f69e2f96933057",
     "grade": true,
     "grade_id": "cell-bc6195f27f0f93b0",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Test accuracy : 0.7560422960725075\n"
     ]
    }
   ],
   "source": [
    "# If you have done everything right, the accuracy should be around 75%\n",
    "lr_pred = preds(lr, tst_vecs)\n",
    "assert lr_pred.shape == (3972,)\n",
    "print(\"Logistic Regression Test accuracy : {}\".format(\n",
    "    accuracy_score(tst_labels, lr_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Change to TfidfVectorizer, and also change the configuration\n",
    "\n",
    "Look up the documentation of [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html). It has a lot of parameters to play with. \n",
    "\n",
    "This time, change the parameters to include _maximum_ of __10000__ features. Also include filtering of _stopwords_ and _lowercasing_ the features. (hint: look at the parameter names in the documentation)\n",
    "\n",
    "Also [_ngram_](https://en.wikipedia.org/wiki/N-gram) features can improve the performance of the model. A bigram is an n-gram for n=2, trigram is when n=3, etc..\n",
    "\n",
    "\n",
    "Bigram features include not only single words in the vocabulary, but the frequency of every occuring bigram in the text (e.g. it will include not only the words _brown_ and _dog_ but __brown dog__ also)\n",
    "\n",
    "Change the configuration of the _TfidfVectorizer_ to also include the _bigrams_ and _trigrams_ in the vocabulary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c811e892327a2cba08939276e67c09c8",
     "grade": false,
     "grade_id": "cell-4e74fe87b4b17324",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def prepare_tfidf_vectorizer(train_data):\n",
    "    vectorizer = TfidfVectorizer(max_features=10000, stop_words=\"english\", lowercase=True, ngram_range=(2, 3))\n",
    "    feature_vector = vectorizer.fit(train_data.tweet)\n",
    "    \n",
    "    return feature_vector\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    #raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f2f6d06c5b75dcd3459f23ec673797e8",
     "grade": false,
     "grade_id": "cell-470b0bb87098c483",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "tr_vecs, tr_labels, tst_vecs, tst_labels = get_features_and_labels(\n",
    "    train_data.tweet, train_data.label, prepare_tfidf_vectorizer(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "abfa8f70571a8ec882f6830492c16906",
     "grade": false,
     "grade_id": "cell-2a89733013d98c1d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Test accuracy : 0.6852970795568983\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate! \n",
    "lr  = LogisticRegression(n_jobs=-1)\n",
    "\n",
    "\n",
    "#lr.fit...\n",
    "lr.fit(tr_vecs, tr_labels)\n",
    "\n",
    "# Evaluation on the test dataset\n",
    "#lr_pred = ..\n",
    "lr_pred = lr.predict(tst_vecs)\n",
    "# YOUR CODE HERE\n",
    "print(\"Logistic Regression Test accuracy : {}\".format(\n",
    "    accuracy_score(tst_labels, lr_pred)))\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ddb3dc0fd6562f838a7ccf31bb32c106",
     "grade": true,
     "grade_id": "cell-58df3ea2b9f1335b",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.utils.validation import check_is_fitted\n",
    "from sklearn.exceptions import NotFittedError\n",
    "\n",
    "try:\n",
    "    check_is_fitted(lr)\n",
    "except NotFittedError as e:\n",
    "    assert None, repr(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Write a custom tokenizer for TfidfVectorizer\n",
    "\n",
    "Right now, the vectorizer uses it's own tokenizer for creating the vocabulary. You can also create a custom function and tell the vectorizer to use that when tokenizing the text.\n",
    "\n",
    "Use [spacy](https://spacy.io/) for tokenization. write your own function.\n",
    "\n",
    "Your function should:\n",
    "- get a sentence as an input\n",
    "- run spacy on the input text\n",
    "- return a token list that includes:\n",
    "    - filtering of stop words\n",
    "    - filtering of punctuation\n",
    "    - lemmatizing the text\n",
    "    - lowercasing the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9458296f3b9931a28cecc11924990e56",
     "grade": false,
     "grade_id": "cell-31035c3e0e78d8f6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nlp', 'lab', 'text', 'contain', 'punctuation', 'stopword', 'text', 'lowercase']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "'''    \n",
    "sen_list=[\"Muffins\", \"cost\"]\n",
    "joint_sen= \" \".join(sen_list) # join a list of words into a big string.\n",
    "                              # reference: https://note.nkmk.me/en/python-string-concat/#:~:text=If%20you%20want%20to%20concatenate%20a%20list%20of%20numbers%20into,concatenate%20them%20with%20join()%20.  \n",
    "'''\n",
    "\n",
    "\n",
    "def spacy_tokenizer(sentence):\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    doc = nlp(sentence) # run the machine learning on the sentence\n",
    "    #tokens = [token.text for token in doc] # create a list to contain all the tokenizers which are recognized by nlp spacy.\n",
    "    tokens_lemma = [token.lemma_ for token in doc] # create a list to contain all the tokenizers which are recognized by nlp spacy\n",
    "                                             # and also lemmatated \n",
    "    \n",
    "    # Create new doc for furthur processing:\n",
    "    new_string=\" \".join(tokens_lemma)\n",
    "    doc1=nlp(new_string)\n",
    "    \n",
    "    \n",
    "    #print(tokens_lemma)\n",
    "    '''\n",
    "    # TESTING SOME FEATURES OF TOKENIZATION IN SPACY:\n",
    "    for token in doc:\n",
    "        print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "            token.shape_, token.is_alpha, token.is_stop, type (token.pos_))\n",
    "    # test the type of token.pos_ => result: string type\n",
    "    #print(lemmata)\n",
    "    print(type(tokens))\n",
    "    '''\n",
    "    \n",
    "   \n",
    "    # Remove stop words:\n",
    "    for token in doc1:\n",
    "        \n",
    "        rw=token.orth_ # reference: https://stackoverflow.com/questions/49889113/converting-spacy-token-vectors-into-text\n",
    "        if token.is_stop == True:\n",
    "            rw=rw.lower()\n",
    "            if (rw in tokens_lemma): # check whether the remove words exist in the list of tokens:\n",
    "                tokens_lemma.remove(rw)\n",
    "        \n",
    "    \n",
    "    # Remove punctuation:\n",
    "    for token in doc1:\n",
    "        #print(type(token))\n",
    "        rw=token.orth_ # reference: https://stackoverflow.com/questions/49889113/converting-spacy-token-vectors-into-text\n",
    "        #print(type(rw))\n",
    "        if token.pos_ == \"PUNCT\" or token.pos_==\"punct\":\n",
    "            if (rw in tokens_lemma):\n",
    "                tokens_lemma.remove(rw)\n",
    "    \n",
    "    # Lowercase the tokens:\n",
    "    lwTokens=[]\n",
    "    for i in range(0, len(tokens_lemma)) :\n",
    "        lwTokens.append(tokens_lemma[i].lower())\n",
    "        \n",
    "    \n",
    "    return lwTokens\n",
    "\n",
    "\n",
    "# SELF_TEST:\n",
    "f=spacy_tokenizer(\"This is the NLP lab, this text should not contain any punctuations and stopwords, and the text should be lowercased.\")\n",
    "print(f)\n",
    "\n",
    "vectorizer_with_spacy = TfidfVectorizer(\n",
    "    max_features=10000, tokenizer=spacy_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b5097da78652b20a0ca2b09ddf4ed454",
     "grade": true,
     "grade_id": "cell-70b92faae3867608",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert (spacy_tokenizer(\"This is the NLP lab, this text should not contain any punctuations and stopwords, and the text should be lowercased.\") == [\n",
    "        'nlp', 'lab', 'text', 'contain', 'punctuation', 'stopword', 'text', 'lowercase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6cd0b906365a12ac9816fb4b716552e4",
     "grade": false,
     "grade_id": "cell-932a31d7a48415ce",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fatima/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    }
   ],
   "source": [
    "X = vectorizer_with_spacy.fit(train_data.tweet)\n",
    "\n",
    "tr_vecs, tr_labels, tst_vecs, tst_labels = get_features_and_labels(train_data.tweet, train_data.label, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7c9ce3e1c9db008b94c8e2a518ea1c9e",
     "grade": false,
     "grade_id": "cell-28cbd9926b7630ca",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Test accuracy : 0.7565458207452165\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate! \n",
    "# If you have done everything right you should get the same or a little better performance than the standard\n",
    "# TfidfVectorizer and CountVectorizer\n",
    "lr  = LogisticRegression(n_jobs=-1) # LogisticRegression is a Classifier\n",
    "\n",
    "#lr.fit...\n",
    "lr.fit(tr_vecs, tr_labels)\n",
    "#lr_pred = ..\n",
    "lr_pred = lr.predict(tst_vecs)\n",
    "print(\"Logistic Regression Test accuracy : {}\".format(\n",
    "    accuracy_score(tst_labels, lr_pred)))\n",
    "# YOUR CODE HERE\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Transform word vectors to sentence vector taking the average of the word vectors\n",
    "Word vectors transform words to a vector space where similar words have similar vectors.\n",
    "These vectors can be used as features for ML algorithms. But to feature a sentence first you need to create a _sentence vector_ from the vectors of the words. The easiest way of transforming word vectors to sentence vector is to take the average of all the word vectors.\n",
    "\n",
    "![ww](https://www.researchgate.net/profile/Md-Shajalal/publication/329394770/figure/fig1/AS:701809937088513@1544335936936/A-framework-for-learning-word-vectors-7_W640.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-29-6ce219125bfe>:5: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  vectorizer = model.wv\n",
      "<ipython-input-29-6ce219125bfe>:6: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  vocab_length = len(model.wv.vocab)\n"
     ]
    }
   ],
   "source": [
    "#Load the embedding file\n",
    "embedding_file = \"glove.6B.100d.txt\"\n",
    "\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(embedding_file, binary=False)\n",
    "vectorizer = model.wv\n",
    "vocab_length = len(model.wv.vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your transform function should:**\n",
    "- tokenize the sentence with the spacy tokenizer\n",
    "- get the embedding vector:\n",
    "    - get the embedding vector from the model if the word is in the vocabulary\n",
    "    - initialize a vector with zeros with the same dimension if the word is not in the vocabulary\n",
    "- take the mean of the word vectors to return a sentence vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'str'>\n",
      "['nlp']\n",
      "<class 'str'>\n",
      "['nlp', 'lab']\n",
      "2\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-30-b89d19ec0bed>:2: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  model.wv.vocab\n",
      "<ipython-input-30-b89d19ec0bed>:3: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  type(model.wv.vocab) # dictionary type\n",
      "<ipython-input-30-b89d19ec0bed>:14: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  if (sen[i] in model.wv.vocab):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['nlp', 'lab']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_length\n",
    "model.wv.vocab\n",
    "type(model.wv.vocab) # dictionary type\n",
    "\n",
    "# CHECK WHETHER WORDS IN THE DICT:\n",
    "#reference: https://able.bio/rhett/check-if-a-key-exists-in-a-python-dictionary--73iajoz#:~:text=To%20simply%20check%20if%20a,')%20%23%20Dogs%20found!&text=A%20dictionary%20can%20be%20a,counting%20the%20occurrence%20of%20items.\n",
    "sen=['nlp', 'lab', 'tapphao', 'lalung']\n",
    "\n",
    "Dct=[]\n",
    "print(type(Dct))\n",
    "inDct=0\n",
    "notDct=0\n",
    "for i in range (0, len(sen)):\n",
    "    if (sen[i] in model.wv.vocab):\n",
    "        inDct= inDct+1\n",
    "        #if inDct==1:\n",
    "        t=str(sen[i])\n",
    "        print(type(t))\n",
    "        Dct.append(t)\n",
    "        print(Dct)\n",
    "        #else:\n",
    "        #    m=np.array(sen[i])\n",
    "        #    Dct=np.concatenate((Dct,m), axis=0)\n",
    "    else:\n",
    "        notDct= notDct +1\n",
    "print(inDct)\n",
    "print(notDct)\n",
    "Dct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "075d84b4a8325a3baaa488afc30d05a3",
     "grade": false,
     "grade_id": "cell-f6d0bba39013a876",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-31-7908281d9427>:13: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  if (tokens_spacy[i] in model.wv.vocab):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def transform(words):\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    tokens_spacy= spacy_tokenizer(words)\n",
    "    #print(tokens_spacy)\n",
    "    \n",
    "    # Filter out which tokens are in the model vocabulary and which are not:\n",
    "\n",
    "    InVocab=[] # new list contains only tokens in the vocabulary\n",
    "    Vocab=0\n",
    "    notVocab=0\n",
    "    for i in range (0, len(tokens_spacy)):\n",
    "            if (tokens_spacy[i] in model.wv.vocab):\n",
    "                Vocab= Vocab+1\n",
    "                t=str(tokens_spacy[i])\n",
    "                InVocab.append(t)\n",
    "                \n",
    "            else: # NotInVocab contains the zero vectors for tokens not the model vocab:\n",
    "                notVocab= notVocab +1\n",
    "                if notVocab ==1 :\n",
    "                    NotInVocab= np.zeros((1,100)) # 100 is used because by default in this model the embedded word vector size is (100,)\n",
    "                else:\n",
    "                    m=np.zeros((1,100))\n",
    "                    NotInVocab= np.concatenate((NotInVocab, m), axis=0)\n",
    "                \n",
    "    \n",
    "    # Embedding tokens which are in the model vocab:\n",
    "    embedded_vector = model[InVocab]\n",
    "    print(embedded_vector[0].shape)\n",
    "    print(type(embedded_vector[0]))\n",
    "    \n",
    "    #The matrix contains all the embedding vectors of all the tokens:\n",
    "    if notVocab == 0:\n",
    "        FinalMatrix= embedded_vector\n",
    "    else:\n",
    "        FinalMatrix=np.concatenate((embedded_vector,NotInVocab), axis=0)\n",
    "        # this matrix has dimension (len(tokens_spacy), 100) #len(tokens_spacy) : total number of words (or tokens) in the list tokens_spacy\n",
    "    \n",
    "    # Get the average vector of all embedding vectors for returning value:\n",
    "    s=0 # used for computing sum of the embedding vectors\n",
    "    for i in range (0, len(tokens_spacy)):\n",
    "        s= s+FinalMatrix[i]\n",
    "    return_vector= s/len(tokens_spacy)\n",
    "    \n",
    "    return return_vector\n",
    "#t=np.zeros((1,2))\n",
    "#m=np.zeros((1,2))\n",
    "#TBlock= np.concatenate((t, m), axis=0)\n",
    "\n",
    "#print(TBlock.shape)\n",
    "r=transform(\"this is a nlp lab \")\n",
    "r.shape\n",
    "r\n",
    "rl=np.array(r)\n",
    "rl.shape\n",
    "rl\n",
    "type(rl)\n",
    "    #raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0533f8b61bf05e8fbb8c0f11261f423c",
     "grade": true,
     "grade_id": "cell-c015012d531b7c3a",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-31-7908281d9427>:13: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  if (tokens_spacy[i] in model.wv.vocab):\n"
     ]
    }
   ],
   "source": [
    "assert transform(\"this is a nlp lab\").shape == (100,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can calculate similarities between sentences now the same way that we did between words! For this we need to use the cosine_similarity function!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-31-7908281d9427>:13: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  if (tokens_spacy[i] in model.wv.vocab):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n",
      "<class 'numpy.ndarray'>\n",
      "(100,)\n",
      "<class 'numpy.ndarray'>\n",
      "0.7142756824337592\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "print(cosine_similarity(transform(\"hello my name is adam\").reshape(\n",
    "    1, -1), transform(\"hello my name is andrea\").reshape(1, -1))[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5947c3827d4c3def144392cac336c108",
     "grade": true,
     "grade_id": "cell-6a71e2b32b6277e8",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n",
      "<class 'numpy.ndarray'>\n",
      "(100,)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-31-7908281d9427>:13: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  if (tokens_spacy[i] in model.wv.vocab):\n"
     ]
    }
   ],
   "source": [
    "assert cosine_similarity(transform(\"hello my name is adam\").reshape(\n",
    "    1, -1), transform(\"hello my name is andrea\").reshape(1, -1)).shape == (1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Finding Analogies\n",
    "Word vectors have been shown to sometimes have the ability to solve analogies.\n",
    "\n",
    "We discussed this during the lecture that for the analogy \"man : king :: woman : x\" (read: man is to king as woman is to x), x is _queen_\n",
    "\n",
    "Find more examples of analogies that holds according to these vectors (i.e. the intended word is ranked top)!\n",
    "\n",
    "Also find an example of analogy that does not hold according to these vectors!\n",
    "\n",
    "Summarize your findings in a few sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7dae55ed9ff89f94f896945692ad39f4",
     "grade": true,
     "grade_id": "cell-a32ae4b69f94e14b",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man is to king as woman is to...\n",
      "[('queen', 0.7698541283607483), ('monarch', 0.6843380928039551), ('throne', 0.6755737066268921), ('daughter', 0.6594556570053101), ('princess', 0.6520533561706543), ('prince', 0.6517034769058228), ('elizabeth', 0.6464517116546631), ('mother', 0.631171703338623), ('emperor', 0.6106470823287964), ('wife', 0.6098655462265015)]\n",
      "\n",
      "\n",
      "Vectors do not hold analogy to these vectors\n",
      "[('___________________________________________________________', 0.7018510699272156), ('ryryryryryry', 0.6911153793334961), ('nanobiotechnology', 0.6836133003234863), ('shoshani', 0.683387041091919), ('tom.fowler@chron.com', 0.6821105480194092), ('soejima', 0.6800203323364258), ('brett.clanton@chron.com', 0.6775467991828918), ('geoinformatics', 0.6769911050796509), ('methoni', 0.6744081377983093), ('zety', 0.6733587384223938)]\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "def analogy(word1, word2, word3, n=5): # to find the similariries between words.\n",
    "    \n",
    "    #get vectors for each word\n",
    "    word1_vector = model[word1]\n",
    "    word2_vector = model[word2]\n",
    "    word3_vector = model[word3]\n",
    "    \n",
    "    #calculate analogy vector\n",
    "    analogy_vector = model.most_similar(positive=[word3, word2], negative=[word1])\n",
    "    \n",
    "    \n",
    "    #calculate not-analogy vector\n",
    "    not_analogy_vector = model.most_similar(negative=[word1, word2, word3])\n",
    "    \n",
    "    print(word1 + \" is to \" + word2 + \" as \" + word3 + \" is to...\")\n",
    "    \n",
    "    return analogy_vector, not_analogy_vector\n",
    "\n",
    "a,n=analogy('man', 'king', 'woman') # the words most analogy to those vectors are\n",
    "# 'queen' cause it has weight  0.7698541283607483 ranked as top\n",
    "# next, 'monarch', 0.6843380928039551\n",
    "# next, 'throne', 0.6755737066268921\n",
    "print(a)\n",
    "print('\\n')\n",
    "print('Vectors do not hold analogy to these vectors') # words really not related to these 'man', 'king', 'woman' vectors:\n",
    "print(n)\n",
    "# rank2 for most not-related : 'ryryryryryry', 0.6911153793334961\n",
    "# next: 'anobiotechnology', 0.6836133003234863\n",
    "# next: 'shoshani', 0.683387041091919\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Bias in word vectors\n",
    "\n",
    "It's important to be cognizant of the biases (gender, race, sexual orientation etc.) implicit in our word embeddings. Bias  in word vectors can be dangerous because it can incorporate stereotypes through applications that employ these models.\n",
    "\n",
    "Run the cell below, to examine a sample of gender bias present in the data. Try to come up with another examples that can reflect biases in datasets (gender, race, sexual orientation etc.)\n",
    "\n",
    "Summarize your findings in a few sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('nurse', 0.7735227346420288), ('physician', 0.7189429998397827), ('doctors', 0.6824328303337097), ('patient', 0.6750683188438416), ('dentist', 0.6726033687591553), ('pregnant', 0.6642460227012634), ('medical', 0.6520450115203857), ('nursing', 0.645348072052002), ('mother', 0.6393327116966248), ('hospital', 0.6387495398521423)]\n",
      "[('dr.', 0.65594881772995), ('brother', 0.6274003982543945), ('him', 0.6236444711685181), ('he', 0.6169813871383667), ('himself', 0.6075390577316284), ('physician', 0.6073206067085266), ('father', 0.5924621820449829), ('master', 0.5798596143722534), ('friend', 0.5763945579528809), ('taken', 0.5739063024520874)]\n"
     ]
    }
   ],
   "source": [
    "print(model.most_similar(positive=['woman', 'doctor'], negative=['man']))\n",
    "\n",
    "print(model.most_similar(positive=['man', 'doctor'], negative=['woman']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0247a46741127cfff3b9de15b4f9a72f",
     "grade": true,
     "grade_id": "cell-7d3945be6a14bd81",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('parents', 0.5668851137161255), ('sister', 0.5646761059761047), ('employer', 0.5568981766700745), ('spouse', 0.5436244010925293), ('mothers', 0.5349369645118713), ('pregnant', 0.53443843126297), ('adult', 0.5250305533409119), ('sibling', 0.5245696902275085), ('provider', 0.5196497440338135), ('mother', 0.5136243104934692)]\n",
      "[('company', 0.6763399839401245), ('subsidiary', 0.6488295793533325), ('unit', 0.6444345712661743), ('group', 0.6404563784599304), ('brothers', 0.6023896932601929), ('owner', 0.582736611366272), ('owned', 0.5813060402870178), ('based', 0.5802786350250244), ('executive', 0.5775868892669678), ('owns', 0.5734580755233765)]\n",
      "\n",
      "\n",
      "[('hesitant', 0.5911028385162354), ('feeble', 0.5903016924858093), ('diffident', 0.5849113464355469), ('introverted', 0.5825530886650085), ('cocky', 0.5734557509422302), ('cheerful', 0.5699166655540466), ('lethargic', 0.5676059126853943), ('indecisive', 0.567166805267334), ('witless', 0.5650360584259033), ('headstrong', 0.5644875764846802)]\n",
      "[('pushy', 0.6496704816818237), ('cynical', 0.6377893090248108), ('arrogant', 0.6265795826911926), ('clumsy', 0.6247188448905945), ('hesitant', 0.623668372631073), ('naive', 0.6227809190750122), ('reticent', 0.6110929846763611), ('foolish', 0.6002066135406494), ('standoffish', 0.5959659814834595), ('feckless', 0.5958832502365112)]\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "print(model.most_similar(positive=['woman', 'parent' ], negative=['man']))\n",
    "\n",
    "print(model.most_similar(positive=['man', 'parent'], negative=['woman']))\n",
    "\n",
    "# The result shows: when 'man' is related to 'parent', the 'woman' relates to ('parents', 'sister', 'spouse ') \n",
    "#which are quite analogy to 'parent'\n",
    "\n",
    "# while 'women' is related to 'parent', the 'man' relates to ('company', 'subsidiary', 'group') \n",
    "# which are really far from the meaning of 'parent'\n",
    "\n",
    "# => CONCLUSION: the distances between ('woman' to 'parent') and ('men' to 'parent') are very different. From the result\n",
    "\n",
    "# the word 'women' sourrounded by very big group of analogy words related 'parent'\n",
    "\n",
    "# while the word 'men' closely sourrounded by gorups of words not analogy to 'parent' (i.e 'company', 'subsidiary', etc), \n",
    "# further than that there is maybe the groups of words analogy to 'parent'\n",
    "\n",
    "\n",
    "# =>> THERE IS A BIAS IN GENDER with the parental work.(Women tends to parent more than men)\n",
    "\n",
    "print('\\n')\n",
    "print(model.most_similar(positive=['asia', 'timid' ], negative=['europe']))\n",
    "\n",
    "print(model.most_similar(positive=['europe', 'timid'], negative=['asia']))\n",
    "\n",
    "# The result shows: when 'europe' is related to 'timid', the 'asia' relates to ('hesitant', 'feeble', 'diffident') \n",
    "#which are quite analogy to 'timid'\n",
    "\n",
    "# while 'asia' is related to 'timid', the 'europe' relates to ('pushy', 'cynical', 'arrogant') \n",
    "# which are really far from the meaning of 'timid'\n",
    "\n",
    "# => CONCLUSION: the distances between ('asia' to 'timid') and ('europe' to 'timid') are very different. From the result\n",
    "\n",
    "# the word 'asia' sourrounded by very big group of analogy words related 'timid'\n",
    "\n",
    "# while the word 'europe' closely sourrounded by gorups of words not too analogy to 'timid' (i.e 'pushy', 'cynical', 'arrogant' etc), \n",
    "# further than that there is maybe the groups of words most analogy to 'timid'\n",
    "\n",
    "\n",
    "# =>> THERE IS A BIAS IN Race within the timid characteristic. (Asia tends to timid more than Europe)\n",
    "\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ================ PASSING LEVEL ===================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Logistic regression using word vectors\n",
    "\n",
    "These sentence vectors can be used as feature vectors for classifiers. Rewrite the featurizing process and transform each sentence into a sentence vector using the embedding model!\n",
    "\n",
    "__Note: it is OK if your model is not better than the other classifiers__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b2d7bd95147f36144b514fcc987d4f01",
     "grade": false,
     "grade_id": "cell-c3d782b99a73d946",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def transform(words):\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    tokens_spacy= spacy_tokenizer(words)\n",
    "    #print(tokens_spacy)\n",
    "    \n",
    "    # Filter out which tokens are in the model vocabulary and which are not:\n",
    "\n",
    "    InVocab=[] # new list contains only tokens in the vocabulary\n",
    "    Vocab=0\n",
    "    notVocab=0\n",
    "    for i in range (0, len(tokens_spacy)):\n",
    "            if (tokens_spacy[i] in model.wv.vocab):\n",
    "                Vocab= Vocab+1\n",
    "                t=str(tokens_spacy[i])\n",
    "                InVocab.append(t)\n",
    "                \n",
    "            else: # NotInVocab contains the zero vectors for tokens not the model vocab:\n",
    "                notVocab= notVocab +1\n",
    "                if notVocab ==1 :\n",
    "                    NotInVocab= np.zeros((1,100)) # 100 is used because by default in this model the embedded word vector size is (100,)\n",
    "                else:\n",
    "                    m=np.zeros((1,100))\n",
    "                    NotInVocab= np.concatenate((NotInVocab, m), axis=0)\n",
    "                \n",
    "    \n",
    "    # Embedding tokens which are in the model vocab:\n",
    "    if Vocab==0:\n",
    "        embedded_vector=np.zeros((1,100))\n",
    "    else:\n",
    "        embedded_vector = model[InVocab]\n",
    "    #print(embedded_vector.shape)\n",
    "    #The matrix contains all the embedding vectors of all the tokens:\n",
    "    if notVocab == 0:\n",
    "        FinalMatrix= embedded_vector\n",
    "    else:\n",
    "        FinalMatrix=np.concatenate((embedded_vector,NotInVocab), axis=0)\n",
    "        # this matrix has dimension (len(tokens_spacy), 100) #len(tokens_spacy) : total number of words (or tokens) in the list tokens_spacy\n",
    "    \n",
    "    # Get the average vector of all embedding vectors for returning value:\n",
    "    s=0 # used for computing sum of the embedding vectors\n",
    "    for i in range (0, len(tokens_spacy)):\n",
    "        s= s+FinalMatrix[i]\n",
    "    return_vector= s/len(tokens_spacy)\n",
    "    \n",
    "    return return_vector # return type is np.narray dimension (100,) (= (1,100))\n",
    "\n",
    "\n",
    "def vectorize_to_embedding(tr_data, tst_data):    \n",
    "    # YOUR CODE HERE\n",
    "    #print(tr_data[0])\n",
    "    #print(type(tr_data[0]))\n",
    "    \n",
    "    # Transform each string object (a sentece) into word embedding vector (using the Function 'transform' in task 2)\n",
    "    count_tr=0;\n",
    "    for i in range (0, len(tr_data)): #MUST CHANGE 4 TO len(tr_data) for actual run\n",
    "        #print(tr_data[i])\n",
    "        output= transform(tr_data[i]).reshape(1,100) # reshape to concatanate them together\n",
    "        #print(\"output shape:\")\n",
    "        #print(output.shape==(1,100))\n",
    "        count_tr=count_tr+1\n",
    "        if count_tr ==1:\n",
    "             tr_WordEmbedVectors= output\n",
    "        else:\n",
    "            tr_WordEmbedVectors= np.concatenate((tr_WordEmbedVectors,output), axis=0)\n",
    "        #print(tr_WordEmbedVectors.shape)\n",
    "            \n",
    "\n",
    "    count_tst=0;\n",
    "    for i in range (0, len(tst_data)): #MUST CHANGE 4 TO len(tr_data) for actual run\n",
    "        output= transform(tst_data[i]).reshape(1,100)\n",
    "        count_tst=count_tst+1\n",
    "        if count_tst ==1:\n",
    "             tst_WordEmbedVectors= output\n",
    "        else:\n",
    "            tst_WordEmbedVectors= np.concatenate((tst_WordEmbedVectors,output), axis=0)\n",
    "        \n",
    "    return tr_WordEmbedVectors, tst_WordEmbedVectors\n",
    "    #raise NotImplementedError()\n",
    "    \n",
    "def get_features_and_labels(data, labels):\n",
    "    # YOUR CODE HERE\n",
    "    #print(data)\n",
    "    \n",
    "    #print(labels)\n",
    "    \n",
    "    try:\n",
    "        tr_data,tst_data,tr_labels,tst_labels = split(data, labels, test_size=0.3, random_state=1234)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    #print(tst_labels.shape)\n",
    "    # MUST CONVERT THE tr_data and tst_data INTO list type for The word embedding vetorize process\n",
    "    process_tr_data=tr_data.tolist()\n",
    "    process_tst_data=tst_data.tolist()\n",
    "    #print( process_tr_data[1])\n",
    "    #print(type(process_tr_data[1]))\n",
    "    # After The word embedding vetorize process, each 'string object' ( each sentence) \n",
    "    # in tr_data or tr_test is converted into a vector dimension (1,100) (100: 100 features), which contains only numerical types.\n",
    "    finish_tr_data, finish_tst_data =vectorize_to_embedding(process_tr_data, process_tst_data)\n",
    "    \n",
    "    #print(finish_tr_data.shape)\n",
    "    #print(finish_tst_data.shape)\n",
    "    #print(\"\\n\")\n",
    "    #print(finish_tr_data[0].shape)\n",
    "    #print(finish_tst_data[0].shape)\n",
    "   \n",
    "    #tst_vecs = []\n",
    "    #tr_vecs = []\n",
    "#    raise NotImplementedError()\n",
    "    #return process_tr_data[1]\n",
    "\n",
    "    return finish_tr_data,tr_labels, finish_tst_data,tst_labels\n",
    "#r=transform(\"hello\")\n",
    "#r.shape\n",
    "#l=r.reshape(1,100)\n",
    "#l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d38ff4466f18e04f6eee8435f83b79fe",
     "grade": true,
     "grade_id": "cell-c87f042243566944",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-38-7a784887b30b>:13: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  if (tokens_spacy[i] in model.wv.vocab):\n"
     ]
    }
   ],
   "source": [
    "tr_vecs, tr_labels, tst_vecs, tst_labels = get_features_and_labels(train_data.tweet, train_data.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cee08cb7dd74cb5111fba7942873eb3a",
     "grade": true,
     "grade_id": "cell-11a9e1ed6def8a1e",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert tr_vecs[0].shape == (100,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2fe1f890e3007c6ca2458d4d8bcd2612",
     "grade": false,
     "grade_id": "cell-5189b74cd4f4372f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9268, 100)\n",
      "(9268,)\n",
      "Logistic Regression Test accuracy : 0.7449647532729103\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate! \n",
    "lr  = LogisticRegression(n_jobs=-1)\n",
    "\n",
    "#lr.fit...\n",
    "print(tr_vecs.shape)\n",
    "print(tr_labels.shape)\n",
    "lr.fit(tr_vecs, tr_labels)\n",
    "\n",
    "#lr_pred = ..\n",
    "lr_pred = lr.predict(tst_vecs)\n",
    "print(\"Logistic Regression Test accuracy : {}\".format(\n",
    "    accuracy_score(tst_labels, lr_pred)))\n",
    "# YOUR CODE HERE\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Ensemble model\n",
    "\n",
    "Try out other classifiers from: [sklearn](https://scikit-learn.org/stable/supervised_learning.html). Choose three and build a [VotingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html) with the choosen classifiers. If the _voting_ strategy is set to _hard_ it will do a majority voting among the classifiers and choose the class with the most votes.\n",
    "\n",
    "Make a [Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) with a TFIdfVectorizer and with your Ensemble model. Pipeline objects make it easy to assemble several steps together and makes your machine learning pipeline executable in just one step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d0bbd431b65ed0c72a51b79f53660b3b",
     "grade": false,
     "grade_id": "cell-496b5cf9c757f418",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier Test accuracy : 0.6143001007049346\n",
      "RandomForestClassifier Test accuracy : 0.6759818731117825\n",
      "AdaBoostClassifier Test accuracy : 0.7225579053373615\n",
      "VotingClassifier Test accuracy : 0.7056898288016112\n"
     ]
    }
   ],
   "source": [
    "# Reference:https://scikit-learn.org/stable/modules/ensemble.html#voting-classifier\n",
    "# Reference: https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html\n",
    "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html\n",
    "\n",
    "# IN GENERAL: the machine leaarning (or more complicated: Deep Learning) contains:\n",
    "# FIRST:\n",
    "# The input layer: at this layer, we must convert (or process) our raw database,\n",
    "# each element of which, into a vector of numerical values \n",
    "# (the number of numerical values depend on how many characteristics (features) we want to use to describe an element, IMPORTANT:\n",
    "# THIS NUMBER OF FEATURES ARE THE NUMBER OF INPUT NODES OF THE INPUT LAYER OF THE MACHINE LEARNING).\n",
    "\n",
    "# SECOND:\n",
    "# the hidden layer: at this layer, there is a classifier which will predict the labels (or classes) of the input vectors.\n",
    "# The prediction ability of the classifer obtains after we have trained the classifier (the heart of the machine learning)\n",
    "# with our training data and our training labels.\n",
    "# For the classifier, there are a variety of classifiers we can choose from in order to satisfy our main requirement \n",
    "# for the outputs (predictions) of the machine learning.\n",
    "\n",
    "\n",
    "# IN TASK 1 AND TASK 2: we have always used logistic Regression classifier\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# There are lots of kinds of classifiers => There are also many categories for these classifiers:\n",
    "# first category: called linear_model which contains i.e 'Logistic Regression' classifier \n",
    "## IN TASK 1 AND TASK 2: we have always used 'logistic Regression' classifier\n",
    "\n",
    "# second category: called naive_bayes which contains i.e 'GaussianNB' classifier\n",
    "\n",
    "# third category: called ensemble which contains i.e 'RandomForestClassifier' classifier, 'Voting classifier'\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn import tree\n",
    "\n",
    "\n",
    "\n",
    "## TRY OUT 4 DIFFERENT CLASSIFIERS BESIDES LOSGISTIC REGRESSION CLASSIFIER:\n",
    "\n",
    "# 'DecisionTreeClassifier' classifier:\n",
    "\n",
    "clf= tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(tr_vecs, tr_labels)\n",
    "clf_pred = clf.predict(tst_vecs) # get the predicted labels of the machine when we use the test data vectors.\n",
    "print(\"DecisionTreeClassifier Test accuracy : {}\".format(\n",
    "    accuracy_score(tst_labels, clf_pred)))\n",
    "\n",
    "\n",
    "#   'RandomForestClassifier' classifier\n",
    "clf1= RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1)\n",
    "clf1 = clf1.fit(tr_vecs, tr_labels)\n",
    "clf1_pred = clf1.predict(tst_vecs)\n",
    "print(\"RandomForestClassifier Test accuracy : {}\".format(\n",
    "    accuracy_score(tst_labels, clf1_pred)))\n",
    "\n",
    "# 'AdaBoostClassifier' classifier:\n",
    "clf2=  AdaBoostClassifier()\n",
    "clf2 = clf2.fit(tr_vecs, tr_labels)\n",
    "clf2_pred = clf2.predict(tst_vecs)\n",
    "print(\"AdaBoostClassifier Test accuracy : {}\".format(\n",
    "    accuracy_score(tst_labels, clf2_pred)))\n",
    "\n",
    "# 'Voting classifier' classifier:\n",
    "eclf1 = VotingClassifier(estimators=[('dstr', clf), ('rdf', clf1), ('ada', clf2)], voting='hard')\n",
    "    # 'dstr', 'rdf', 'ada' are just how we want to name these classifiers, we can change those names to different ones \n",
    "    # without affecting the VotingClassifier.\n",
    "eclf1 = eclf1.fit(tr_vecs, tr_labels)\n",
    "eclf1_pred = eclf1.predict(tst_vecs)\n",
    "print(\"VotingClassifier Test accuracy : {}\".format(\n",
    "    accuracy_score(tst_labels, eclf1_pred)))\n",
    "\n",
    "\n",
    "# Pipline declration: pipe = Pipeline([<step 1>, <step 2>])\n",
    "# Pipline only\n",
    "# combines the all the steps: (step 1: how to process the raw data (i.e bag of word, word embedding, etc))\n",
    "# and (step 2: choose the classifier) into one command.\n",
    "# However, this command only demonstrates our steps, we do not apply any real inputs for these steps,\n",
    "# which is equivalent to our raw data is still not vetorized, our classifier is not trained.\n",
    "# As a consequent, we have to apply the inputs (train data and the train labels) to the pipline as follow:\n",
    "# pipe.fit(trained data, train label)\n",
    "# After that we can also evaluate the prediction of our model:\n",
    "#pipe.score(test_data, preictions_of_machine)\n",
    "def make_pipeline_ensemble(tweet, label):\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    # declaration:\n",
    "    pipe=Pipeline([('Tfid', TfidfVectorizer(max_features=10000, stop_words=\"english\", lowercase=True, ngram_range=(2, 3))), \n",
    "                   ('VoteClass', VotingClassifier(estimators=[('dstr', clf), ('rdf', clf1), ('ada', clf2)]))])\n",
    "    \n",
    "    # train the machine:\n",
    "    \n",
    "    return pipe\n",
    "    #raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a2aa00a09ac0821d059b76920daf77e5",
     "grade": false,
     "grade_id": "cell-91e45b470dd2fdc2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "pipeline = make_pipeline_ensemble(train_data.tweet, train_data.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1883ed3bcab25408ae1c3570c80ad5d4",
     "grade": true,
     "grade_id": "cell-ac56617dcdd3842a",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(pipeline) == Pipeline\n",
    "assert type(pipeline.steps[0][1]) == TfidfVectorizer\n",
    "assert type(pipeline.steps[1][1]) == VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6abb4535a19cf53912cda707a07c97ea",
     "grade": false,
     "grade_id": "cell-4d271a643404ed6b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensmeble_machine Test accuracy : 0.6840382678751259\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate! \n",
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "# Prepare the train data, test data, train label, test labe from the raw data\n",
    "raw_tr_data,raw_tst_data,raw_tr_labels,raw_tst_labels = split(train_data.tweet, train_data.label, test_size=0.3, random_state=1234)\n",
    "\n",
    "# train:\n",
    "pipeline.fit(raw_tr_data,raw_tr_labels)\n",
    "\n",
    "# evaluate:\n",
    "pipeline_pred = pipeline.predict(raw_tst_data)\n",
    "print(\"Ensmeble_machine Test accuracy : {}\".format(\n",
    "    accuracy_score(raw_tst_labels, pipeline_pred)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 __Also evaluate your classifiers separately as well. Summarize your results in a cell below. Did the ensemble model improved your performance?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9d4cfd5c8edfd5725cf3f460efb37440",
     "grade": true,
     "grade_id": "cell-5fedea728b48ee0a",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier Test accuracy : 0.6092648539778449\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# 'DecisionTreeClassifier' classifier: (use vectorized data by Word Embedding Vectorization in task 3)\n",
    "\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(tr_vecs, tr_labels)\n",
    "clf_pred = clf.predict(tst_vecs) # get the predicted labels of the machine when we use the test data vectors.\n",
    "print(\"DecisionTreeClassifier Test accuracy : {}\".format(\n",
    "    accuracy_score(tst_labels, clf_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier Test accuracy : 0.6779959718026183\n"
     ]
    }
   ],
   "source": [
    "# 'RandomForestClassifier' classifier\n",
    "clf1= RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1)\n",
    "clf1 = clf1.fit(tr_vecs, tr_labels)\n",
    "clf1_pred = clf1.predict(tst_vecs)\n",
    "print(\"RandomForestClassifier Test accuracy : {}\".format(\n",
    "    accuracy_score(tst_labels, clf1_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier Test accuracy : 0.7225579053373615\n"
     ]
    }
   ],
   "source": [
    "#'AdaBoostClassifier' classifier: (use vectorized data by Word Embedding Vectorization in task 3)\n",
    "clf2=  AdaBoostClassifier()\n",
    "clf2 = clf2.fit(tr_vecs, tr_labels)\n",
    "clf2_pred = clf2.predict(tst_vecs)\n",
    "print(\"AdaBoostClassifier Test accuracy : {}\".format(\n",
    "    accuracy_score(tst_labels, clf2_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VotingClassifier Test accuracy : 0.7046827794561934\n"
     ]
    }
   ],
   "source": [
    "# 'Voting classifier' classifier: (use vectorized data by Word Embedding Vectorization in task 3)\n",
    "eclf1 = VotingClassifier(estimators=[('dstr', clf), ('rdf', clf1), ('ada', clf2)], voting='hard')\n",
    "    # 'dstr', 'rdf', 'ada' are just how we want to name these classifiers, we can change those names to different ones \n",
    "    # without affecting the VotingClassifier.\n",
    "eclf1 = eclf1.fit(tr_vecs, tr_labels)\n",
    "eclf1_pred = eclf1.predict(tst_vecs)\n",
    "print(\"VotingClassifier Test accuracy : {}\".format(\n",
    "    accuracy_score(tst_labels, eclf1_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensmeble_machine Test accuracy : 0.6840382678751259\n"
     ]
    }
   ],
   "source": [
    "# The ensemble_machine (pipeline) (the pipeline uses vectorized data by TfidfVectorizer (= pipeline[0][1])):\n",
    "pipeline_pred = pipeline.predict(raw_tst_data)\n",
    "print(\"Ensmeble_machine Test accuracy : {}\".format(\n",
    "    accuracy_score(raw_tst_labels, pipeline_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CONCLUSION:\n",
    "# the machine learning made of Word Embedding Vectorizatioin and AdaBoostClassifier gives out the best prediction accuracy: 0.7225579053373615\n",
    "\n",
    "# the machine made by 'pipeline' gives out fairly accuracy (rank 3/5) :  0.6842900302114804"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ================ EXTRA LEVEL ===================="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
