{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"07_Deep_learning_nlp_lab - Copy.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true}},"cells":[{"cell_type":"markdown","metadata":{"id":"HCLtpQQia2RB"},"source":["# Introduction to Python and Natural Language Technologies\n","\n","__Laboratory 07, Deep learning and NLP__\n","\n","__March 25, 2021__\n","\n","__Ádám Kovács__\n"]},{"cell_type":"markdown","metadata":{"id":"OZVQi-fAa2RI"},"source":["During this laboratory we are going to use the same classification dataset that we used the last time: SemEval 2019 - Task 6. \n","The dataset is about Identifying and Categorizing Offensive Language in Social Media.\n","__Preparation:__\n","- You will need the Semeval dataset (we will have code to download it)\n","- You will need to install pytorch:\n","    - pip install torch \n","- You will also need to have pandas, torchtext, numpy and scikit learn installed, you can find the instructions for them in the lecture notebook."]},{"cell_type":"markdown","metadata":{"id":"G5YuROa8a2RI"},"source":["We are going to use an open source library for building optimized deep learning models that can be run on GPUs, the library is called [Pytorch](https://pytorch.org/docs/stable/index.html). It is one of the most widely used libraries for building neural networks/deep learning models.\n","\n","__NOTE: If your notebook/PC is not good enough, it is advised to use Google Colab for this laboratory for free access to GPUs. If you have completed the exercises, you can download the notebook and upload it to the repository__"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4gWZp0RFa2RJ","executionInfo":{"status":"ok","timestamp":1616676651984,"user_tz":-60,"elapsed":3907,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"33a9f6c2-223d-48b1-9aa4-b8bd88f18eda"},"source":["!pip install torch"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.8.0+cu101)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch) (1.19.5)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Id9ntsvPa2RJ","executionInfo":{"status":"ok","timestamp":1616703635801,"user_tz":-60,"elapsed":642,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}}},"source":["# Import the needed libraries\n","import pandas as pd\n","import numpy as np"],"execution_count":234,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"heIeVCGta2RK"},"source":["## 0. Download the dataset and load it into a pandas DataFrame\n","\n","__Note: you can reuse your code from the previous lab!__"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3AaIOd3ca2RK","executionInfo":{"status":"ok","timestamp":1616703660539,"user_tz":-60,"elapsed":2101,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"95fe8dad-53d5-4d48-9d76-7ff2a0c6a238"},"source":["# First we download the data using the code from last week\n","import os\n","if not os.path.isdir('./data'):\n","    os.mkdir('./data')\n","\n","import urllib.request # modified for python version 3.7\n","u = urllib.request.URLopener()\n","u.retrieve(\"http://sandbox.hlt.bme.hu/~adaamko/offenseval.tsv\",\n","           \"data/offenseval.tsv\")"],"execution_count":236,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('data/offenseval.tsv', <http.client.HTTPMessage at 0x7f787fd6b110>)"]},"metadata":{"tags":[]},"execution_count":236}]},{"cell_type":"markdown","metadata":{"id":"f5IbuDKqa2RK"},"source":["## 0.1 Read in the dataset into a Pandas DataFrame\n","Use `pd.read_csv` with the correct parameters to read in the dataset. If done correctly, `DataFrame` should have 3 columns, \n","`id`, `tweet`, `subtask_a`."]},{"cell_type":"code","metadata":{"id":"A0xvntjja2RK","executionInfo":{"status":"ok","timestamp":1616703663806,"user_tz":-60,"elapsed":695,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}}},"source":["import pandas as pd\n","import numpy as np"],"execution_count":237,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"f7bedd1169acf1b3d4471815f3317cdc","grade":false,"grade_id":"cell-eef320fdacfdf485","locked":false,"schema_version":3,"solution":true,"task":false},"colab":{"base_uri":"https://localhost:8080/","height":415},"id":"EL5MQO2Na2RL","executionInfo":{"status":"ok","timestamp":1616703667929,"user_tz":-60,"elapsed":644,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"f892b938-d99f-4bd1-dd8a-76cbc4be3c18"},"source":["def read_dataset():\n","    # YOUR CODE HERE:\n","\n","    #SAME AS LAB6:\n","    dataset = pd.read_csv(\"./data/offenseval.tsv\",sep=\"\\t\", names = [\"id\", \"tweet\", \"subtask_a\"])\n","    final_dataset= dataset.iloc[1:]\n","    return final_dataset\n","    #raise NotImplementedError()\n","d= read_dataset()\n","d"],"execution_count":238,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>tweet</th>\n","      <th>subtask_a</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1</th>\n","      <td>86426</td>\n","      <td>@USER She should ask a few native Americans wh...</td>\n","      <td>OFF</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>90194</td>\n","      <td>@USER @USER Go home you’re drunk!!! @USER #MAG...</td>\n","      <td>OFF</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>16820</td>\n","      <td>Amazon is investigating Chinese employees who ...</td>\n","      <td>NOT</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>62688</td>\n","      <td>@USER Someone should'veTaken\" this piece of sh...</td>\n","      <td>OFF</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>43605</td>\n","      <td>@USER @USER Obama wanted liberals &amp;amp; illega...</td>\n","      <td>NOT</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>13236</th>\n","      <td>95338</td>\n","      <td>@USER Sometimes I get strong vibes from people...</td>\n","      <td>OFF</td>\n","    </tr>\n","    <tr>\n","      <th>13237</th>\n","      <td>67210</td>\n","      <td>Benidorm ✅  Creamfields ✅  Maga ✅   Not too sh...</td>\n","      <td>NOT</td>\n","    </tr>\n","    <tr>\n","      <th>13238</th>\n","      <td>82921</td>\n","      <td>@USER And why report this garbage.  We don't g...</td>\n","      <td>OFF</td>\n","    </tr>\n","    <tr>\n","      <th>13239</th>\n","      <td>27429</td>\n","      <td>@USER Pussy</td>\n","      <td>OFF</td>\n","    </tr>\n","    <tr>\n","      <th>13240</th>\n","      <td>46552</td>\n","      <td>#Spanishrevenge vs. #justice #HumanRights and ...</td>\n","      <td>NOT</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>13240 rows × 3 columns</p>\n","</div>"],"text/plain":["          id                                              tweet subtask_a\n","1      86426  @USER She should ask a few native Americans wh...       OFF\n","2      90194  @USER @USER Go home you’re drunk!!! @USER #MAG...       OFF\n","3      16820  Amazon is investigating Chinese employees who ...       NOT\n","4      62688  @USER Someone should'veTaken\" this piece of sh...       OFF\n","5      43605  @USER @USER Obama wanted liberals &amp; illega...       NOT\n","...      ...                                                ...       ...\n","13236  95338  @USER Sometimes I get strong vibes from people...       OFF\n","13237  67210  Benidorm ✅  Creamfields ✅  Maga ✅   Not too sh...       NOT\n","13238  82921  @USER And why report this garbage.  We don't g...       OFF\n","13239  27429                                        @USER Pussy       OFF\n","13240  46552  #Spanishrevenge vs. #justice #HumanRights and ...       NOT\n","\n","[13240 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":238}]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"080fac8757293be3f2a8eabe0f733645","grade":true,"grade_id":"cell-8f39b3b86623648c","locked":true,"points":0,"schema_version":3,"solution":false,"task":false},"id":"O4Z1N7sLa2RL","executionInfo":{"status":"ok","timestamp":1616703673189,"user_tz":-60,"elapsed":587,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}}},"source":["train_data_unprocessed = read_dataset()\n","\n","assert type(train_data_unprocessed) == pd.core.frame.DataFrame\n","assert len(train_data_unprocessed.columns) == 3\n","assert (train_data_unprocessed.columns == ['id', 'tweet', 'subtask_a']).all()"],"execution_count":239,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j-AzJT0Sa2RL"},"source":["## 0.2 Convert `subtask_a` into a binary label\n","The task is to classify the given tweets into two category: _offensive(OFF)_ , _not offensive (NOT)_. For machine learning algorithms you will need integer labels instead of strings. Add a new column to the dataframe called `label`, and transform the `subtask_a` column into a binary integer label."]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"2d884573710457f4fc0ebd549fc19646","grade":false,"grade_id":"cell-595b437c85da4194","locked":false,"schema_version":3,"solution":true,"task":false},"colab":{"base_uri":"https://localhost:8080/","height":415},"id":"bK4qowaha2RM","executionInfo":{"status":"ok","timestamp":1616703681676,"user_tz":-60,"elapsed":1003,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"dd2d7adf-4ae9-458a-f250-1aa85bea8b81"},"source":["def transform(train_data):\n","    # YOUR CODE HERE\n","    # SAME AS LAB 6:\n","    train_data[\"label\"]=train_data.subtask_a.apply(lambda x: 1 if x== \"NOT\" else 0 )\n","    return train_data\n","    #raise NotImplementedError()\n","tr_d=transform(d)\n","tr_d"],"execution_count":240,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>tweet</th>\n","      <th>subtask_a</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1</th>\n","      <td>86426</td>\n","      <td>@USER She should ask a few native Americans wh...</td>\n","      <td>OFF</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>90194</td>\n","      <td>@USER @USER Go home you’re drunk!!! @USER #MAG...</td>\n","      <td>OFF</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>16820</td>\n","      <td>Amazon is investigating Chinese employees who ...</td>\n","      <td>NOT</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>62688</td>\n","      <td>@USER Someone should'veTaken\" this piece of sh...</td>\n","      <td>OFF</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>43605</td>\n","      <td>@USER @USER Obama wanted liberals &amp;amp; illega...</td>\n","      <td>NOT</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>13236</th>\n","      <td>95338</td>\n","      <td>@USER Sometimes I get strong vibes from people...</td>\n","      <td>OFF</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>13237</th>\n","      <td>67210</td>\n","      <td>Benidorm ✅  Creamfields ✅  Maga ✅   Not too sh...</td>\n","      <td>NOT</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>13238</th>\n","      <td>82921</td>\n","      <td>@USER And why report this garbage.  We don't g...</td>\n","      <td>OFF</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>13239</th>\n","      <td>27429</td>\n","      <td>@USER Pussy</td>\n","      <td>OFF</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>13240</th>\n","      <td>46552</td>\n","      <td>#Spanishrevenge vs. #justice #HumanRights and ...</td>\n","      <td>NOT</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>13240 rows × 4 columns</p>\n","</div>"],"text/plain":["          id  ... label\n","1      86426  ...     0\n","2      90194  ...     0\n","3      16820  ...     1\n","4      62688  ...     0\n","5      43605  ...     1\n","...      ...  ...   ...\n","13236  95338  ...     0\n","13237  67210  ...     1\n","13238  82921  ...     0\n","13239  27429  ...     0\n","13240  46552  ...     1\n","\n","[13240 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":240}]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"840a03cb2fd64a51bed889bd0a35c699","grade":true,"grade_id":"cell-56fa86b834804581","locked":true,"points":0,"schema_version":3,"solution":false,"task":false},"id":"kG_qzq6Ya2RM","executionInfo":{"status":"ok","timestamp":1616703688312,"user_tz":-60,"elapsed":701,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}}},"source":["from pandas.api.types import is_numeric_dtype\n","\n","train_data = transform(train_data_unprocessed)\n","\n","assert \"label\" in train_data\n","assert is_numeric_dtype(train_data.label)\n","assert (train_data.label.isin([0,1])).all()"],"execution_count":241,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hlq0I-6Sa2RM"},"source":["## 1. Train a simple neural network on this dataset\n","\n","__HINT: you can reuse the code from the Lecture! Most of the code will be very similar that we used there!__"]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"08bf496f087259962194259ba91d929d","grade":false,"grade_id":"cell-8c242629b22cb523","locked":true,"schema_version":3,"solution":false,"task":false},"colab":{"base_uri":"https://localhost:8080/"},"id":"36istvtEa2RM","executionInfo":{"status":"ok","timestamp":1616703691634,"user_tz":-60,"elapsed":605,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"b5652c1f-1ea6-4656-c1c7-f5b5bdaa6b0a"},"source":["#Import pytorch and set a fixed random seed for reproducibility\n","import torch\n","\n","SEED = 1234\n","\n","torch.manual_seed(SEED)\n","torch.backends.cudnn.deterministic = True\n","type(SEED)"],"execution_count":242,"outputs":[{"output_type":"execute_result","data":{"text/plain":["int"]},"metadata":{"tags":[]},"execution_count":242}]},{"cell_type":"markdown","metadata":{"id":"WBWbrcaca2RM"},"source":["### 1.1 Split the dataset into a train and a validation dataset\n","Use the random seed for splitting. You should split the dataset into 70% training data and 30% validation data"]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"a85b6e66ec6abc2a655bc78b380f7873","grade":false,"grade_id":"cell-20ba609174c640e7","locked":false,"schema_version":3,"solution":true,"task":false},"id":"O_Av5BCla2RN","executionInfo":{"status":"ok","timestamp":1616703695237,"user_tz":-60,"elapsed":598,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}}},"source":["from sklearn.model_selection import train_test_split as split\n","\n","def split_data(train_data, random_seed):\n","    # YOUR CODE HERE\n","  tr_data, val_data = split(train_data, test_size=0.3, random_state=random_seed)\n","  return tr_data, val_data\n","    #raise NotImplementedError()\n","tr_data, val_data = split_data(train_data, SEED)"],"execution_count":243,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"fbacfe8507716ba6e631ebcb2db1d042","grade":true,"grade_id":"cell-0e8a125310d3fea9","locked":true,"points":0,"schema_version":3,"solution":false,"task":false},"id":"03JfBdWIa2RN","executionInfo":{"status":"ok","timestamp":1616703697413,"user_tz":-60,"elapsed":841,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}}},"source":["tr_data, val_data = split_data(train_data, SEED)\n","assert len(tr_data) == 9268"],"execution_count":244,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1dzcsyOga2RN"},"source":["### 1.2 Use CountVectorizer to prepare the features for the sentences\n","You should fit CountVectorizer using _10000_ features"]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"1d61d01db26026ffa90eeff6b8448c16","grade":false,"grade_id":"cell-c0943811065a971f","locked":false,"schema_version":3,"solution":true,"task":false},"id":"MJJw98PWa2RN","executionInfo":{"status":"ok","timestamp":1616703701420,"user_tz":-60,"elapsed":939,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}}},"source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","def prepare_vectorizer(tr_data):\n","    # YOUR CODE HERE\n","  vectorizer = CountVectorizer(max_features=10000)\n","  word_to_ix = vectorizer.fit(tr_data.tweet)\n","  return word_to_ix\n","    #raise NotImplementedError()\n"],"execution_count":245,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"d0d3a7512761645c10f85b3159a243b5","grade":true,"grade_id":"cell-a2c6658aef3041dc","locked":true,"points":0,"schema_version":3,"solution":false,"task":false},"id":"4AygpZcPa2RN","executionInfo":{"status":"ok","timestamp":1616703705938,"user_tz":-60,"elapsed":926,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}}},"source":["word_to_ix = prepare_vectorizer(tr_data)\n","VOCAB_SIZE = len(word_to_ix.vocabulary_)\n","assert VOCAB_SIZE == 10000"],"execution_count":246,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1MYE5O_wa2RO"},"source":["### 1.3 Prepare the DataLoader for batch processing\n","\n","The __prepare_dataloader(..)__ function will take the training and the validation dataset and convert them to one-hot encoded vectors with the help of the initialized CountVectorizer.\n","\n","You should prepare two FloatTensor for the converted tweets of the training and the validation data.\n","\n","Then zip together the vectors with the labels as a list of tuples!\n","\n","__Hint: look at the lecture (but be careful, we had different types of labels there!)__"]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"e2e1374a1e11decbf349c0b213237c12","grade":false,"grade_id":"cell-67b120b4ea6ba288","locked":false,"schema_version":3,"solution":true,"task":false},"id":"5EBuzGfLa2RO","executionInfo":{"status":"ok","timestamp":1616703710043,"user_tz":-60,"elapsed":843,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}}},"source":["def prepare_dataloader(tr_data, val_data, word_to_ix):\n","    # YOUR CODE HERE\n","  tr_data_vecs = torch.FloatTensor(word_to_ix.transform(tr_data.tweet).toarray())\n","  tr_labels = tr_data.label.tolist()\n","\n","  val_data_vecs = torch.FloatTensor(word_to_ix.transform(val_data.tweet).toarray())\n","  val_labels = val_data.label.tolist()\n","\n","  # Answer from teacher Kovac Adam:\n","  # In the lecture: labels range from [1,2,3,4] => must fix the range into [0,1,2,3] for later usage\n","  # => (sample, label-1) is used\n","  # in this case, the labels range from [0,1] => already correct with the index requirement\n","  # => no need for label-1. IN this case if label -1 is kept, there would be INDEX ERROR at 1.5 Task below:\n","  \n","\n","  tr_data_loader = [(sample, label) for sample, label in zip(tr_data_vecs, tr_labels)] \n","  val_data_loader = [(sample, label) for sample, label in zip(val_data_vecs, val_labels)]\n","  return tr_data_loader, val_data_loader\n","    #raise NotImplementedError()"],"execution_count":247,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"d8bdab9604ed014c79d99e6e03487be2","grade":true,"grade_id":"cell-212fb18e207761c4","locked":true,"points":0,"schema_version":3,"solution":false,"task":false},"id":"801fiJEGa2RO","executionInfo":{"status":"ok","timestamp":1616703716590,"user_tz":-60,"elapsed":3245,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}}},"source":["tr_data_loader, val_data_loader = prepare_dataloader(tr_data, val_data, word_to_ix)\n","assert type(tr_data_loader[0][0]) == torch.Tensor\n","assert len(tr_data_loader) == 9268\n","assert type(tr_data_loader[0][1]) == int"],"execution_count":248,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CxEfrrWma2RP"},"source":["- __We have the correct lists now, it is time to initialize the DataLoader objects!__\n","- __Create two DataLoader objects with the lists we have created__\n","- __Shuffle the training data but not the validation data!__\n","- __Set a BATCH_SIZE, experiment with different sized batches to see if it improves the performance__"]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"618ed179b22bea85eac18e721acce473","grade":false,"grade_id":"cell-96ac025a45bc4fec","locked":false,"schema_version":3,"solution":true,"task":false},"id":"Gx-ioYAva2RQ","executionInfo":{"status":"ok","timestamp":1616703721180,"user_tz":-60,"elapsed":597,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}}},"source":["from torch.utils.data import DataLoader\n","\n","def create_dataloader_iterators(tr_data_loader, val_data_loader, BATCH_SIZE):\n","    # YOUR CODE HERE\n","  train_iterator = DataLoader(tr_data_loader,\n","                            batch_size=BATCH_SIZE,\n","                            shuffle=True,\n","                            )\n","  \n","  valid_iterator = DataLoader(val_data_loader,\n","                          batch_size=BATCH_SIZE,\n","                          shuffle=False,\n","                          )\n","  return train_iterator, valid_iterator\n","    #raise NotImplementedError()"],"execution_count":249,"outputs":[]},{"cell_type":"code","metadata":{"id":"QbhlbwLva2RR","executionInfo":{"status":"ok","timestamp":1616703727106,"user_tz":-60,"elapsed":736,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}}},"source":["# Try to experiment with different sized batches and see if changing this will improve the performance or not!\n","BATCH_SIZE = 64\n","\n","# The affect of changing BATCH_SIZE, i will do this procedure at task 2.1 "],"execution_count":250,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"060b970b69ce6aca6b3661b10c9537fc","grade":true,"grade_id":"cell-7b88321ec3ee1096","locked":true,"points":0,"schema_version":3,"solution":false,"task":false},"id":"oA_4Z_xza2RR","executionInfo":{"status":"ok","timestamp":1616703731763,"user_tz":-60,"elapsed":641,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}}},"source":["train_iterator, valid_iterator = create_dataloader_iterators(tr_data_loader, val_data_loader, BATCH_SIZE)\n","assert type(train_iterator) == torch.utils.data.dataloader.DataLoader"],"execution_count":251,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"34b2b98bb33063d9d2053af9bbf0ad7c","grade":false,"grade_id":"cell-4d1819598c663eb1","locked":true,"schema_version":3,"solution":false,"task":false},"id":"ILls21fZa2RR","executionInfo":{"status":"ok","timestamp":1616703745480,"user_tz":-60,"elapsed":1110,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}}},"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"execution_count":252,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zGrdh7Voa2RS"},"source":["### 1.4 Build the model\n","At first, the model only should contain a single Linear layer that takes one-hot-encoded vectors and trainsforms it into the dimension if the __NUM_LABELS__(how many classes we are trying to predict). Then, run through the output on a softmax activation to produce probabilites of the classes!"]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"6981cb10b5c0db3d3e870ae6e7bbe533","grade":false,"grade_id":"cell-00fb572132edf99a","locked":false,"schema_version":3,"solution":true,"task":false},"id":"EiNFh5sKa2RT","executionInfo":{"status":"ok","timestamp":1616703751252,"user_tz":-60,"elapsed":1736,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}}},"source":["from torch import nn\n","\n","class BoWClassifier(nn.Module):  # inheriting from nn.Module!\n","    # YOUR CODE HERE\n","    def __init__(self, num_labels, vocab_size):\n","        # calls the init function of nn.Module.  Dont get confused by syntax,\n","        # just always do it in an nn.Module\n","        super(BoWClassifier, self).__init__()\n","\n","        \n","        self.linear = nn.Linear(vocab_size, num_labels) # vocab_size: the input layer (the number of input nodes = the number of features of the input vector)\n","                                                        # number_labels: the output layer (the number of output nodes= the number of labels or classes (in our case, there are 4 labels: sport, world,..))\n","\n","    def forward(self, bow_vec):\n","        return F.log_softmax(self.linear(bow_vec), dim=1)\n","    #raise NotImplementedError()"],"execution_count":253,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"93181cc0bf9bb4abcc13fc0d18234c78","grade":false,"grade_id":"cell-3cbec9b993598632","locked":false,"schema_version":3,"solution":true,"task":false},"id":"ni5-TtJ0a2RT","executionInfo":{"status":"ok","timestamp":1616703755908,"user_tz":-60,"elapsed":1160,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}}},"source":["# SET THE CORRECT INPUT AND OUTPUT DIMENSIONS!\n","INPUT_DIM = 10000\n","OUTPUT_DIM = 2\n","# YOUR CODE HERE\n","#raise NotImplementedError()"],"execution_count":254,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"f55e912ebd6e352c4e025c1a88a42b78","grade":true,"grade_id":"cell-203697b21306ccb9","locked":true,"points":0,"schema_version":3,"solution":false,"task":false},"id":"yoD2ezeka2RU","executionInfo":{"status":"ok","timestamp":1616703758211,"user_tz":-60,"elapsed":998,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}}},"source":["model = BoWClassifier(OUTPUT_DIM, INPUT_DIM)"],"execution_count":255,"outputs":[]},{"cell_type":"code","metadata":{"id":"1Yc2-ntUa2RU","executionInfo":{"status":"ok","timestamp":1616703760783,"user_tz":-60,"elapsed":1096,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}}},"source":["# Set the optimizer and the loss function!\n","import torch.optim as optim\n","\n","optimizer = optim.Adam(model.parameters(), lr=1e-3)\n","criterion = nn.NLLLoss()"],"execution_count":256,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"41ba901b5c558f48029035996ac4f262","grade":true,"grade_id":"cell-9852177c09074615","locked":true,"points":0,"schema_version":3,"solution":false,"task":false},"id":"g8unUgbea2RU","executionInfo":{"status":"ok","timestamp":1616703764498,"user_tz":-60,"elapsed":1574,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}}},"source":["model = model.to(device)\n","criterion = criterion.to(device)"],"execution_count":257,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"bcec3fb4b3490966859ad9e78baeeb6b","grade":true,"grade_id":"cell-50c925cbe0576fd3","locked":true,"points":0,"schema_version":3,"solution":false,"task":false},"id":"lQq_iRFQa2RU","executionInfo":{"status":"ok","timestamp":1616703767361,"user_tz":-60,"elapsed":1082,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}}},"source":["assert model.linear.in_features == 10000\n","assert model.linear.out_features == 2"],"execution_count":258,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mSZ1D7_ra2RU"},"source":["### Implement the following functions:\n","- __calculate_performance__: This should calculate the batch-wise accuracy of your model!\n","- __train__ - Train your model on the training data! This function should set the model to training mode, then use the given iterator to iterate through the training samples and make predictions using the provided model. You should then propagate back the error with the loss function and the optimizer. Finally return the average epoch loss and accuracy!\n","- __evaluate__ - Evaluate your model on the validation dataset. This function is essentially the same as the trainnig function, but you should set your model to eval mode and don't propagate back the errors to your weights!"]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"764cb398380a9ed1e1b98a857c8f312f","grade":false,"grade_id":"cell-1818f7b4bce37196","locked":false,"schema_version":3,"solution":true,"task":false},"id":"39ZRTokka2RV","executionInfo":{"status":"ok","timestamp":1616703774427,"user_tz":-60,"elapsed":660,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}}},"source":["def calculate_performance(preds, y):\n","    # YOUR CODE HERE\n","    rounded_preds = preds.argmax(1)\n","    # Calculate the correct predictions batch-wise\n","    correct = (rounded_preds == y).float()\n","    \n","    # Calculate the accuracy of your model\n","    acc = correct.sum() / len(correct)\n","    return acc\n","    #raise NotImplementedError()"],"execution_count":259,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"b60b63cad118ec51ff9fd7cd130693cf","grade":false,"grade_id":"cell-ea8beb3df906f9a4","locked":false,"schema_version":3,"solution":true,"task":false},"id":"RqnBoab1a2RV","executionInfo":{"status":"ok","timestamp":1616703778345,"user_tz":-60,"elapsed":589,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}}},"source":["import torch.nn.functional as F\n","def train(model, iterator, optimizer, criterion):\n","    epoch_loss = 0\n","    epoch_acc = 0\n","    # YOUR CODE HERE\n","    model.train()\n","    \n","    # We calculate the error on batches so the iterator will return matrices with shape [BATCH_SIZE, VOCAB_SIZE]\n","    for texts, labels in iterator: # an iterator = one batch has gone through the model, and the model weighs has got one update\n","                                   # number of iteration = total samples/ batch size.\n","        # We copy the text and label to the correct device\n","        texts = texts.to(device)\n","        labels = labels.to(device)\n","        \n","        # We reset the gradients from the last step, so the loss will be calculated correctly (and not added together)\n","        optimizer.zero_grad() # the weights are updated on this batch.\n","                              # the loss of the this batch will not affect how the weights in the next batch is updated.  \n","                \n","        # This runs the forward function on your model (you don't need to call it directly)    \n","        predictions = model(texts)\n","\n","        # Calculate the loss and the accuracy on the predictions (the predictions are log probabilities, remember!)\n","        loss = criterion(predictions, labels)\n","        acc = calculate_performance(predictions, labels)\n","        \n","        # Propagate the error back on the model (this means changing the initial weights in your model)\n","        loss.backward()\n","        optimizer.step()\n","        \n","        # We add batch-wise loss to the epoch-wise loss\n","        epoch_loss += loss.item()\n","        epoch_acc += acc.item()\n","    #raise NotImplementedError()\n","    return epoch_loss / len(iterator), epoch_acc / len(iterator)"],"execution_count":260,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"476cd1a61b57394b15c94bfef6666540","grade":false,"grade_id":"cell-810fadb1db8e2028","locked":false,"schema_version":3,"solution":true,"task":false},"id":"85067Pgca2RV","executionInfo":{"status":"ok","timestamp":1616703786008,"user_tz":-60,"elapsed":685,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}}},"source":["def evaluate(model, iterator, criterion):\n","    \n","    epoch_loss = 0\n","    epoch_acc = 0\n","    # YOUR CODE HERE\n","     # On the validation dataset we don't want training so we need to set the model on evaluation mode\n","    model.eval() # do not learn on the data, only give the predictions\n","    \n","    # Also tell Pytorch to not propagate any error backwards in the model\n","    # This is needed when you only want to make predictions and use your model in inference mode!\n","    with torch.no_grad(): # means The we do not update the weights and biases (or the model parameters) anymore with backpropagation\n","                          # = only gives out the predictions, no training or learning procedures anymore.  \n","    \n","        # The remaining part is the same with the difference of not using the optimizer to backpropagation\n","        for texts, labels in iterator: # number of iteration is independent of batch size.\n","          \n","            # We copy the text and label to the correct device\n","            texts = texts.to(device)\n","            labels = labels.to(device)\n","            \n","            predictions = model(texts)\n","            loss = criterion(predictions, labels)\n","            \n","            acc = calculate_performance(predictions, labels)\n","\n","            epoch_loss += loss.item()\n","            epoch_acc += acc.item()\n","    \n","    return epoch_loss / len(iterator), epoch_acc / len(iterator)"],"execution_count":261,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"67a35d2dc61592ed8e60eaad946fcb1b","grade":false,"grade_id":"cell-73c8635f8fc4a7fd","locked":true,"schema_version":3,"solution":false,"task":false},"id":"5zOQx_yGa2RV","executionInfo":{"status":"ok","timestamp":1616703789699,"user_tz":-60,"elapsed":713,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}}},"source":["import time\n","\n","def epoch_time(start_time, end_time):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs"],"execution_count":262,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RIXdt-Oma2RW"},"source":["### 1.5 Training loop!\n","Below is the training loop of our model! Try to set an EPOCH number that will correctly train your model :) (it is not underfitted but neither overfitted!"]},{"cell_type":"code","metadata":{"id":"YnVD6Pu1a2RW","executionInfo":{"status":"ok","timestamp":1616703799285,"user_tz":-60,"elapsed":590,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}}},"source":["# Set an EPOCH number!\n","# one EPOCH= one time the model has been trained with the data\n","N_EPOCHS = 15 # This is the appropriate epoch numbers because the last two epochs show the training loss increased\n","# while the validation loss does not change (= stagnate)"],"execution_count":263,"outputs":[]},{"cell_type":"code","metadata":{"id":"RsKw2ieTa2RW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616703809824,"user_tz":-60,"elapsed":7725,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"c72c27b7-0780-409b-8ad8-b7a9c2fb9ac4"},"source":["best_valid_loss = float('inf')\n","\n","for epoch in range(N_EPOCHS):\n","\n","    start_time = time.time()\n","    \n","    train_loss, train_score = train(model, train_iterator, optimizer, criterion)\n","    valid_loss, valid_score = evaluate(model, valid_iterator, criterion)\n","    \n","    end_time = time.time()\n","\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","    \n","    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n","    print(f'\\tTrain Loss: {train_loss:.3f} | Train Fscore: {train_score*100:.2f}%')\n","    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Fscore: {valid_score*100:.2f}%')"],"execution_count":264,"outputs":[{"output_type":"stream","text":["Epoch: 01 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.638 | Train Fscore: 66.81%\n","\t Val. Loss: 0.616 |  Val. Fscore: 68.20%\n","Epoch: 02 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.580 | Train Fscore: 70.68%\n","\t Val. Loss: 0.592 |  Val. Fscore: 70.34%\n","Epoch: 03 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.539 | Train Fscore: 74.61%\n","\t Val. Loss: 0.577 |  Val. Fscore: 71.75%\n","Epoch: 04 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.507 | Train Fscore: 77.67%\n","\t Val. Loss: 0.565 |  Val. Fscore: 72.02%\n","Epoch: 05 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.480 | Train Fscore: 79.52%\n","\t Val. Loss: 0.557 |  Val. Fscore: 72.77%\n","Epoch: 06 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.457 | Train Fscore: 81.06%\n","\t Val. Loss: 0.551 |  Val. Fscore: 73.26%\n","Epoch: 07 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.437 | Train Fscore: 82.25%\n","\t Val. Loss: 0.546 |  Val. Fscore: 73.64%\n","Epoch: 08 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.420 | Train Fscore: 83.40%\n","\t Val. Loss: 0.542 |  Val. Fscore: 74.45%\n","Epoch: 09 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.404 | Train Fscore: 84.76%\n","\t Val. Loss: 0.539 |  Val. Fscore: 74.43%\n","Epoch: 10 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.390 | Train Fscore: 85.17%\n","\t Val. Loss: 0.537 |  Val. Fscore: 74.63%\n","Epoch: 11 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.377 | Train Fscore: 85.89%\n","\t Val. Loss: 0.536 |  Val. Fscore: 74.65%\n","Epoch: 12 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.365 | Train Fscore: 86.53%\n","\t Val. Loss: 0.535 |  Val. Fscore: 74.73%\n","Epoch: 13 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.354 | Train Fscore: 87.18%\n","\t Val. Loss: 0.534 |  Val. Fscore: 74.88%\n","Epoch: 14 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.344 | Train Fscore: 87.67%\n","\t Val. Loss: 0.535 |  Val. Fscore: 74.88%\n","Epoch: 15 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.335 | Train Fscore: 88.26%\n","\t Val. Loss: 0.535 |  Val. Fscore: 74.85%\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"m3bqYRqTa2RW"},"source":["### 1.6 Change calculate_performance to calculate FScore instead of accuracy\n","\n","Our dataset is very imbalanced. We have twice as many NOT offensive tweets as offensive ones. Accuracy is not a good measure for this.\n","\n","See https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html for fscore calculation.\n","\n","You should expect a heavy drop in performance when you calculate fscore instead of accuracy!\n","\n","__NOTE: DON'T FORGET TO RERUN THE MODEL INITIALIZATION WHEN YOU ARE TRYING TO RUN THE MODEL MULTIPLE TIMES. IF YOU DON'T REINITIALIZE THE MODEL IT WILL CONTINUE THE TRAINING WHERE IT HAS STOPPED LAST TIME AND DOESN'T RUN FROM SRATCH!__\n","\n","These lines:\n","\n","\n","`model = BoWClassifier(OUTPUT_DIM, INPUT_DIM)\n","optimizer = optim.Adam(model.parameters(), lr=1e-3)\n","criterion = nn.NLLLoss()\n","model = model.to(device)\n","criterion = criterion.to(device)`\n","\n","This will reinitialize the model!"]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"300765d5e628b8e8ab48830d8b536bda","grade":false,"grade_id":"cell-53bd197214cb7ae1","locked":false,"schema_version":3,"solution":true,"task":false},"id":"wLisCZE5a2RX","executionInfo":{"status":"ok","timestamp":1616704097275,"user_tz":-60,"elapsed":586,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}}},"source":["from sklearn.metrics import f1_score\n","\n","def calculate_performance(preds, y):\n","    # YOUR CODE HERE\n","    # ROUNDING the prediction values of 'preds'\n","    # if this rounding step is not implemented, there is an error for runing 'f1_score' command\n","    rounded_preds = preds.argmax(1)\n","\n","    # because 'y' and 'rounded_preds' are torch.Tensor types\n","    # this type is not appropriate as f1_score input\n","    # MUST CONVERT THEM TO list type:\n","    y=y.tolist()\n","    rounded_preds=rounded_preds.tolist()\n","\n","    t=f1_score(y, rounded_preds, average='weighted')\n","    return t\n","  "],"execution_count":265,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s5nAxORqGyan","executionInfo":{"status":"ok","timestamp":1616704124154,"user_tz":-60,"elapsed":11697,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"498fd712-8fec-488c-836e-cf6e2d5fadab"},"source":["# RE-INITIATE THE MODEL FOR EACH TRY:\n","model = BoWClassifier(OUTPUT_DIM, INPUT_DIM) \n","optimizer = optim.Adam(model.parameters(), lr=1e-3) \n","criterion = nn.NLLLoss() \n","model = model.to(device) \n","criterion = criterion.to(device)\n","#########\n","for epoch in range(N_EPOCHS):\n","\n","    start_time = time.time()\n","    \n","    train_loss, train_score = train(model, train_iterator, optimizer, criterion)\n","    valid_loss, valid_score = evaluate(model, valid_iterator, criterion)\n","    \n","    end_time = time.time()\n","\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","    \n","    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n","    print(f'\\tTrain Loss: {train_loss:.3f} | Train Fscore: {train_score*100:.2f}%')\n","    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Fscore: {valid_score*100:.2f}%')"],"execution_count":266,"outputs":[{"output_type":"stream","text":["Epoch: 01 | Epoch Time: 0m 1s\n","\tTrain Loss: 0.640 | Train Fscore: 54.11%\n","\t Val. Loss: 0.617 |  Val. Fscore: 56.71%\n","Epoch: 02 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.580 | Train Fscore: 61.55%\n","\t Val. Loss: 0.593 |  Val. Fscore: 61.52%\n","Epoch: 03 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.539 | Train Fscore: 68.84%\n","\t Val. Loss: 0.576 |  Val. Fscore: 65.53%\n","Epoch: 04 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.506 | Train Fscore: 73.86%\n","\t Val. Loss: 0.566 |  Val. Fscore: 67.73%\n","Epoch: 05 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.479 | Train Fscore: 76.99%\n","\t Val. Loss: 0.557 |  Val. Fscore: 67.95%\n","Epoch: 06 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.456 | Train Fscore: 78.91%\n","\t Val. Loss: 0.550 |  Val. Fscore: 69.67%\n","Epoch: 07 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.437 | Train Fscore: 80.73%\n","\t Val. Loss: 0.545 |  Val. Fscore: 70.23%\n","Epoch: 08 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.419 | Train Fscore: 81.82%\n","\t Val. Loss: 0.542 |  Val. Fscore: 71.05%\n","Epoch: 09 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.404 | Train Fscore: 83.06%\n","\t Val. Loss: 0.539 |  Val. Fscore: 71.73%\n","Epoch: 10 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.390 | Train Fscore: 84.25%\n","\t Val. Loss: 0.537 |  Val. Fscore: 71.94%\n","Epoch: 11 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.377 | Train Fscore: 84.94%\n","\t Val. Loss: 0.535 |  Val. Fscore: 72.12%\n","Epoch: 12 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.365 | Train Fscore: 85.77%\n","\t Val. Loss: 0.535 |  Val. Fscore: 72.12%\n","Epoch: 13 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.354 | Train Fscore: 86.38%\n","\t Val. Loss: 0.534 |  Val. Fscore: 72.91%\n","Epoch: 14 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.344 | Train Fscore: 87.20%\n","\t Val. Loss: 0.534 |  Val. Fscore: 73.46%\n","Epoch: 15 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.334 | Train Fscore: 87.66%\n","\t Val. Loss: 0.535 |  Val. Fscore: 73.62%\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"aH-ebu1yGzH5","executionInfo":{"status":"ok","timestamp":1616687872455,"user_tz":-60,"elapsed":549,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}}},"source":[""],"execution_count":102,"outputs":[]},{"cell_type":"code","metadata":{"id":"KHIqnXneGzdl"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1hpjV3kHa2RX"},"source":["## 2. Add more linear layers to your model and experiment with other hyperparameters"]},{"cell_type":"markdown","metadata":{"id":"fZxN5JWHa2RY"},"source":["### 2.1 More layers\n","\n","Currently we only have a single linear layers in our model. Try to add one or more additional linear layers to the model.\n","You should introduce a HIDDEN_SIZE parameter that will be the size of the intermediate representation between the linear layers. Also add a RELU activation function between the linear layers.\n","\n","See more:\n","- https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html\n","- https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_nn.html"]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"ee7963c7c78d974e0fa2a91a8ac0cfbf","grade":false,"grade_id":"cell-f71eea2d6e70ad97","locked":false,"schema_version":3,"solution":true,"task":false},"id":"UF6y3YeEa2RY","executionInfo":{"status":"ok","timestamp":1616704183860,"user_tz":-60,"elapsed":620,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}}},"source":["from torch import nn\n","\n","class BoWDeepClassifier(nn.Module):  # inheriting from nn.Module!\n","    def __init__(self, num_labels, vocab_size, hidden_size):\n","        # YOUR CODE HERE\n","        super(BoWDeepClassifier, self).__init__()\n","\n","        # add one more hidden layer:\n","        self.sequence= nn.Sequential(\n","                    nn.Linear(vocab_size, hidden_size),\n","                    nn.ReLU(),\n","                    nn.Linear(hidden_size,num_labels),\n","                    nn.LogSoftmax(dim=1)\n","        )\n","\n","        #raise NotImplementedError()\n","\n","    def forward(self, bow_vec):\n","        # YOUR CODE HERE\n","        output= self.sequence(bow_vec)\n","        return output\n","        #raise NotImplementedError()"],"execution_count":267,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"6918e45cc07041b50f3bb407783629ab","grade":false,"grade_id":"cell-d3fe8cbd6981f0cb","locked":false,"schema_version":3,"solution":true,"task":false},"id":"nB7TNb-ha2RY"},"source":["# YOUR CODE HERE\n","raise NotImplementedError()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iBe7m_3ca2RZ"},"source":["### Write down your experiences with changing the parameters to the cell below"]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"e386c8f333461f3c953fe7dcf54a5699","grade":false,"grade_id":"cell-fc6038db2048a478","locked":false,"schema_version":3,"solution":true,"task":false},"id":"1cb9n176a2RZ"},"source":["# YOUR CODE HERE\n","'''\n","HIDDEN_SIZE = 200\n","learning_rate = 0.001\n","BATCH_SIZE = 15\n","N_EPOCHS = 15\n","\n","=>Train Loss: 0.012 | Train Fscore: 99.72%\n","\t Val. Loss: 1.482 |  Val. Fscore: 72.54%\n","#########\n","HIDDEN_SIZE = 200\n","learning_rate = 0.001\n","BATCH_SIZE = 64\n","N_EPOCHS = 15\n","\n","=> The final result : Train Loss: 0.011 | Train Fscore: 99.74%\n","\t Val. Loss: 1.497 |  Val. Fscore: 72.03%\n","\n","With one hidden_layer added, the train loss decreased => train score increased\n","while validation loss increased => validation score decreased\n","###########\n","HIDDEN_SIZE = 200\n","learning_rate = 0.001\n","BATCH_SIZE = 100\n","N_EPOCHS = 15\n","\n","=> Train Loss: 0.012 | Train Fscore: 99.73%\n","\t Val. Loss: 1.469 |  Val. Fscore: 72.65%\n","\n","##############\n","HIDDEN_SIZE = 200\n","learning_rate = 0.001\n","BATCH_SIZE = 100\n","N_EPOCHS = 6\n","\n","=>Train Loss: 0.059 | Train Fscore: 98.52%\n","\t Val. Loss: 0.994 |  Val. Fscore: 72.87%\n","\n","#############\n","HIDDEN_SIZE = 230\n","learning_rate = 0.001\n","BATCH_SIZE = 100\n","N_EPOCHS = 6\n","\n","=>Train Loss: 0.049 | Train Fscore: 98.81%\n","\t Val. Loss: 1.036 |  Val. Fscore: 72.48%\n","###########\n","HIDDEN_SIZE = 180\n","learning_rate = 0.001\n","BATCH_SIZE = 100\n","N_EPOCHS = 6\n","\n","=>Train Loss: 0.057 | Train Fscore: 98.66%\n","\t Val. Loss: 0.994 |  Val. Fscore: 72.36%\n","\n","=>> Decrease epochs, decrase epoch to appropriate value, can raise the validation score => model becomes less overfit\n","=>> Decrease/Increase hidden_size => decrase/increase train score but for validation score in both cases always decreases\n","=>> Decrease/Increase BATCH_SIZE => decrease/ increase train score and validation score\n","'''\n","#raise NotImplementedError()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3C96qYQta2RZ","executionInfo":{"status":"ok","timestamp":1616704254938,"user_tz":-60,"elapsed":587,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}}},"source":["HIDDEN_SIZE = 200\n","learning_rate = 0.001\n","BATCH_SIZE = 64\n","N_EPOCHS = 15"],"execution_count":268,"outputs":[]},{"cell_type":"code","metadata":{"id":"5aL8La7ja2RZ","executionInfo":{"status":"ok","timestamp":1616704257736,"user_tz":-60,"elapsed":608,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}}},"source":["model = BoWDeepClassifier(OUTPUT_DIM, INPUT_DIM, HIDDEN_SIZE)\n","\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","criterion = nn.NLLLoss()\n","\n","model = model.to(device)\n","criterion = criterion.to(device)"],"execution_count":269,"outputs":[]},{"cell_type":"code","metadata":{"id":"sC67k-yca2RZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616704356181,"user_tz":-60,"elapsed":95878,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"ed588155-51ea-4ba5-8e85-05ad196ba492"},"source":["\n","\n","# TRAINING LOOP HERE!\n","best_valid_loss = float('inf')\n","\n","for epoch in range(N_EPOCHS):\n","\n","    start_time = time.time()\n","    \n","    train_loss, train_score = train(model, train_iterator, optimizer, criterion)\n","    valid_loss, valid_score = evaluate(model, valid_iterator, criterion)\n","    \n","    end_time = time.time()\n","\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","    \n","    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n","    print(f'\\tTrain Loss: {train_loss:.3f} | Train Fscore: {train_score*100:.2f}%')\n","    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Fscore: {valid_score*100:.2f}%')"],"execution_count":270,"outputs":[{"output_type":"stream","text":["Epoch: 01 | Epoch Time: 0m 6s\n","\tTrain Loss: 0.600 | Train Fscore: 62.30%\n","\t Val. Loss: 0.545 |  Val. Fscore: 71.33%\n","Epoch: 02 | Epoch Time: 0m 5s\n","\tTrain Loss: 0.397 | Train Fscore: 81.75%\n","\t Val. Loss: 0.563 |  Val. Fscore: 74.13%\n","Epoch: 03 | Epoch Time: 0m 5s\n","\tTrain Loss: 0.243 | Train Fscore: 90.38%\n","\t Val. Loss: 0.678 |  Val. Fscore: 73.12%\n","Epoch: 04 | Epoch Time: 0m 5s\n","\tTrain Loss: 0.150 | Train Fscore: 94.78%\n","\t Val. Loss: 0.769 |  Val. Fscore: 72.75%\n","Epoch: 05 | Epoch Time: 0m 5s\n","\tTrain Loss: 0.090 | Train Fscore: 97.37%\n","\t Val. Loss: 0.879 |  Val. Fscore: 72.67%\n","Epoch: 06 | Epoch Time: 0m 5s\n","\tTrain Loss: 0.058 | Train Fscore: 98.43%\n","\t Val. Loss: 0.996 |  Val. Fscore: 72.17%\n","Epoch: 07 | Epoch Time: 0m 6s\n","\tTrain Loss: 0.040 | Train Fscore: 99.02%\n","\t Val. Loss: 1.073 |  Val. Fscore: 72.22%\n","Epoch: 08 | Epoch Time: 0m 6s\n","\tTrain Loss: 0.031 | Train Fscore: 99.20%\n","\t Val. Loss: 1.162 |  Val. Fscore: 72.17%\n","Epoch: 09 | Epoch Time: 0m 6s\n","\tTrain Loss: 0.024 | Train Fscore: 99.43%\n","\t Val. Loss: 1.200 |  Val. Fscore: 72.35%\n","Epoch: 10 | Epoch Time: 0m 6s\n","\tTrain Loss: 0.020 | Train Fscore: 99.49%\n","\t Val. Loss: 1.285 |  Val. Fscore: 72.36%\n","Epoch: 11 | Epoch Time: 0m 6s\n","\tTrain Loss: 0.017 | Train Fscore: 99.64%\n","\t Val. Loss: 1.333 |  Val. Fscore: 71.70%\n","Epoch: 12 | Epoch Time: 0m 6s\n","\tTrain Loss: 0.015 | Train Fscore: 99.56%\n","\t Val. Loss: 1.403 |  Val. Fscore: 72.62%\n","Epoch: 13 | Epoch Time: 0m 6s\n","\tTrain Loss: 0.014 | Train Fscore: 99.72%\n","\t Val. Loss: 1.436 |  Val. Fscore: 72.04%\n","Epoch: 14 | Epoch Time: 0m 6s\n","\tTrain Loss: 0.013 | Train Fscore: 99.69%\n","\t Val. Loss: 1.507 |  Val. Fscore: 72.41%\n","Epoch: 15 | Epoch Time: 0m 6s\n","\tTrain Loss: 0.011 | Train Fscore: 99.74%\n","\t Val. Loss: 1.497 |  Val. Fscore: 72.03%\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8KDOl7XRa2Ra"},"source":["# ================ PASSING LEVEL ===================="]},{"cell_type":"markdown","metadata":{"id":"IDARvyhYa2Rc"},"source":["## 3. Implement automatic early-stopping in the training loop\n","Early stopping is a very easy method to avoid the overfitting of your model.\n","\n","You should:\n","- Save the training and the validation loss of the last two epochs (if you are atleast in the third epoch)\n","- If the loss increased in the last two epoch on the training data but descreased or stagnated in the validation data, you should stop the training automatically!"]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"9e8ae1c966d0e788bab3e69926eee2ac","grade":false,"grade_id":"cell-7d460cc4aa3dd437","locked":false,"schema_version":3,"solution":true,"task":false},"id":"wMUGldgha2Rc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616705140106,"user_tz":-60,"elapsed":175818,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"b3824d32-aa36-4810-9224-40191994adfd"},"source":["HIDDEN_SIZE = 200\n","learning_rate = 0.001\n","BATCH_SIZE = 64\n","N_EPOCHS = 45\n","\n","model = BoWDeepClassifier(OUTPUT_DIM, INPUT_DIM, HIDDEN_SIZE)\n","\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","criterion = nn.NLLLoss()\n","\n","model = model.to(device)\n","criterion = criterion.to(device)\n","\n","\n","# YOUR CODE HERE\n","tr_loss_0=0\n","val_loss_0=0\n","tr_loss_1=0\n","val_loss_1=0\n","for epoch in range(N_EPOCHS):\n","\n","    start_time = time.time()\n","    \n","    train_loss, train_score = train(model, train_iterator, optimizer, criterion)\n","    valid_loss, valid_score = evaluate(model, valid_iterator, criterion)\n","    if epoch == 0:\n","      tr_loss_0 = train_loss\n","      val_loss_0 = valid_loss\n","    if epoch == 1 :\n","      tr_loss_1 = train_loss\n","      val_loss_1 = valid_loss\n","    if epoch > 1:\n","      if (tr_loss_1 - tr_loss_0) > 0 and (val_loss_1 - val_loss_0) <= 0:  \n","        break\n","      else:\n","        tr_loss_0 = tr_loss_1\n","        val_loss_0 = val_loss_1\n","        tr_loss_1 = train_loss\n","        val_loss_1 = valid_loss\n","\n","    end_time = time.time()\n","\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","    \n","    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n","    print(f'\\tTrain Loss: {train_loss:.3f} | Train Fscore: {train_score*100:.2f}%')\n","    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Fscore: {valid_score*100:.2f}%')\n","#raise NotImplementedError()"],"execution_count":272,"outputs":[{"output_type":"stream","text":["Epoch: 01 | Epoch Time: 0m 6s\n","\tTrain Loss: 0.597 | Train Fscore: 61.43%\n","\t Val. Loss: 0.545 |  Val. Fscore: 70.43%\n","Epoch: 02 | Epoch Time: 0m 5s\n","\tTrain Loss: 0.402 | Train Fscore: 81.63%\n","\t Val. Loss: 0.557 |  Val. Fscore: 73.81%\n","Epoch: 03 | Epoch Time: 0m 5s\n","\tTrain Loss: 0.246 | Train Fscore: 90.13%\n","\t Val. Loss: 0.669 |  Val. Fscore: 73.75%\n","Epoch: 04 | Epoch Time: 0m 5s\n","\tTrain Loss: 0.147 | Train Fscore: 94.84%\n","\t Val. Loss: 0.768 |  Val. Fscore: 73.31%\n","Epoch: 05 | Epoch Time: 0m 5s\n","\tTrain Loss: 0.088 | Train Fscore: 97.44%\n","\t Val. Loss: 0.876 |  Val. Fscore: 72.92%\n","Epoch: 06 | Epoch Time: 0m 5s\n","\tTrain Loss: 0.055 | Train Fscore: 98.52%\n","\t Val. Loss: 1.018 |  Val. Fscore: 72.71%\n","Epoch: 07 | Epoch Time: 0m 6s\n","\tTrain Loss: 0.038 | Train Fscore: 99.06%\n","\t Val. Loss: 1.079 |  Val. Fscore: 73.02%\n","Epoch: 08 | Epoch Time: 0m 7s\n","\tTrain Loss: 0.028 | Train Fscore: 99.35%\n","\t Val. Loss: 1.175 |  Val. Fscore: 72.42%\n","Epoch: 09 | Epoch Time: 0m 7s\n","\tTrain Loss: 0.023 | Train Fscore: 99.40%\n","\t Val. Loss: 1.228 |  Val. Fscore: 73.00%\n","Epoch: 10 | Epoch Time: 0m 7s\n","\tTrain Loss: 0.022 | Train Fscore: 99.46%\n","\t Val. Loss: 1.281 |  Val. Fscore: 72.53%\n","Epoch: 11 | Epoch Time: 0m 7s\n","\tTrain Loss: 0.016 | Train Fscore: 99.63%\n","\t Val. Loss: 1.343 |  Val. Fscore: 72.97%\n","Epoch: 12 | Epoch Time: 0m 7s\n","\tTrain Loss: 0.015 | Train Fscore: 99.69%\n","\t Val. Loss: 1.383 |  Val. Fscore: 71.73%\n","Epoch: 13 | Epoch Time: 0m 7s\n","\tTrain Loss: 0.015 | Train Fscore: 99.70%\n","\t Val. Loss: 1.436 |  Val. Fscore: 72.69%\n","Epoch: 14 | Epoch Time: 0m 7s\n","\tTrain Loss: 0.012 | Train Fscore: 99.71%\n","\t Val. Loss: 1.476 |  Val. Fscore: 72.06%\n","Epoch: 15 | Epoch Time: 0m 7s\n","\tTrain Loss: 0.013 | Train Fscore: 99.70%\n","\t Val. Loss: 1.535 |  Val. Fscore: 72.51%\n","Epoch: 16 | Epoch Time: 0m 7s\n","\tTrain Loss: 0.011 | Train Fscore: 99.71%\n","\t Val. Loss: 1.521 |  Val. Fscore: 71.35%\n","Epoch: 17 | Epoch Time: 0m 7s\n","\tTrain Loss: 0.010 | Train Fscore: 99.74%\n","\t Val. Loss: 1.574 |  Val. Fscore: 72.33%\n","Epoch: 18 | Epoch Time: 0m 7s\n","\tTrain Loss: 0.010 | Train Fscore: 99.76%\n","\t Val. Loss: 1.582 |  Val. Fscore: 72.39%\n","Epoch: 19 | Epoch Time: 0m 7s\n","\tTrain Loss: 0.011 | Train Fscore: 99.72%\n","\t Val. Loss: 1.597 |  Val. Fscore: 72.06%\n","Epoch: 20 | Epoch Time: 0m 7s\n","\tTrain Loss: 0.010 | Train Fscore: 99.72%\n","\t Val. Loss: 1.653 |  Val. Fscore: 72.03%\n","Epoch: 21 | Epoch Time: 0m 7s\n","\tTrain Loss: 0.010 | Train Fscore: 99.72%\n","\t Val. Loss: 1.654 |  Val. Fscore: 71.85%\n","Epoch: 22 | Epoch Time: 0m 7s\n","\tTrain Loss: 0.010 | Train Fscore: 99.76%\n","\t Val. Loss: 1.686 |  Val. Fscore: 71.86%\n","Epoch: 23 | Epoch Time: 0m 7s\n","\tTrain Loss: 0.010 | Train Fscore: 99.74%\n","\t Val. Loss: 1.757 |  Val. Fscore: 72.13%\n","Epoch: 24 | Epoch Time: 0m 7s\n","\tTrain Loss: 0.009 | Train Fscore: 99.75%\n","\t Val. Loss: 1.722 |  Val. Fscore: 72.06%\n","Epoch: 25 | Epoch Time: 0m 7s\n","\tTrain Loss: 0.015 | Train Fscore: 99.71%\n","\t Val. Loss: 1.687 |  Val. Fscore: 71.87%\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ovEMtiO3a2Rc"},"source":["## 4. Handling class imbalance\n","Our data is imbalanced, the first class has twice the population of the second class.\n","\n","One way of handling imbalanced data is to weight the loss function, so it penalizes errors on the smaller class.\n","\n","Look at the documentation of the loss function: https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html\n","\n","Set the weights based on the inverse population of the classes (so the less sample a class has, more the errors will be penalized!)"]},{"cell_type":"code","metadata":{"id":"tPCTr3yJa2Rc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616705244128,"user_tz":-60,"elapsed":630,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"3d348df5-0e9b-4226-940f-944a3db8fcff"},"source":["tr_data.groupby(\"label\").size()"],"execution_count":273,"outputs":[{"output_type":"execute_result","data":{"text/plain":["label\n","0    3089\n","1    6179\n","dtype: int64"]},"metadata":{"tags":[]},"execution_count":273}]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"9ce44283629736735331ce2899961f78","grade":false,"grade_id":"cell-6ebf131781a3332d","locked":false,"schema_version":3,"solution":true,"task":false},"id":"-xlfyoeTa2Rd","executionInfo":{"status":"ok","timestamp":1616705247966,"user_tz":-60,"elapsed":838,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}}},"source":["# YOUR CODE HERE\n","model = BoWDeepClassifier(OUTPUT_DIM, INPUT_DIM, HIDDEN_SIZE)\n","\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Modified to deal with unbalance training data\n","# because label 0 has 3089 samples < label 1 has 6179 samples\n","# => the weight of label 0 should be bigger than the weight of label 1\n","# 1/3089 is the weight of label 0\n","# 1/6179 is the weight of label 1\n","criterion = nn.NLLLoss(weight=torch.Tensor([ (1/3089) , (1/6179)]) )\n","\n","model = model.to(device)\n","criterion = criterion.to(device)\n","#raise NotImplementedError()"],"execution_count":274,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fjBB_vC_Apda","executionInfo":{"status":"ok","timestamp":1616705386738,"user_tz":-60,"elapsed":134760,"user":{"displayName":"AIMachineLearningDeepLearning TRAN","photoUrl":"","userId":"04129049261771224104"}},"outputId":"ad36313b-3593-46de-97c7-3be4922ce204"},"source":["tr_loss_0=0\n","val_loss_0=0\n","tr_loss_1=0\n","val_loss_1=0\n","for epoch in range(N_EPOCHS):\n","\n","    start_time = time.time()\n","    \n","    train_loss, train_score = train(model, train_iterator, optimizer, criterion)\n","    valid_loss, valid_score = evaluate(model, valid_iterator, criterion)\n","    if epoch == 0:\n","      tr_loss_0 = train_loss\n","      val_loss_0 = valid_loss\n","    if epoch == 1 :\n","      tr_loss_1 = train_loss\n","      val_loss_1 = valid_loss\n","    if epoch > 1:\n","      if (tr_loss_1 - tr_loss_0) > 0 and (val_loss_1 - val_loss_0) <= 0:  \n","        break\n","      else:\n","        tr_loss_0 = tr_loss_1\n","        val_loss_0 = val_loss_1\n","        tr_loss_1 = train_loss\n","        val_loss_1 = valid_loss\n","\n","    end_time = time.time()\n","\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","    \n","    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n","    print(f'\\tTrain Loss: {train_loss:.3f} | Train Fscore: {train_score*100:.2f}%')\n","    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Fscore: {valid_score*100:.2f}%')"],"execution_count":275,"outputs":[{"output_type":"stream","text":["Epoch: 01 | Epoch Time: 0m 6s\n","\tTrain Loss: 0.641 | Train Fscore: 65.40%\n","\t Val. Loss: 0.592 |  Val. Fscore: 70.38%\n","Epoch: 02 | Epoch Time: 0m 5s\n","\tTrain Loss: 0.418 | Train Fscore: 83.37%\n","\t Val. Loss: 0.627 |  Val. Fscore: 71.01%\n","Epoch: 03 | Epoch Time: 0m 5s\n","\tTrain Loss: 0.248 | Train Fscore: 91.01%\n","\t Val. Loss: 0.754 |  Val. Fscore: 73.11%\n","Epoch: 04 | Epoch Time: 0m 5s\n","\tTrain Loss: 0.144 | Train Fscore: 95.39%\n","\t Val. Loss: 0.907 |  Val. Fscore: 72.94%\n","Epoch: 05 | Epoch Time: 0m 5s\n","\tTrain Loss: 0.090 | Train Fscore: 97.43%\n","\t Val. Loss: 1.056 |  Val. Fscore: 72.29%\n","Epoch: 06 | Epoch Time: 0m 6s\n","\tTrain Loss: 0.058 | Train Fscore: 98.61%\n","\t Val. Loss: 1.187 |  Val. Fscore: 72.01%\n","Epoch: 07 | Epoch Time: 0m 7s\n","\tTrain Loss: 0.050 | Train Fscore: 98.86%\n","\t Val. Loss: 1.200 |  Val. Fscore: 72.03%\n","Epoch: 08 | Epoch Time: 0m 7s\n","\tTrain Loss: 0.033 | Train Fscore: 99.28%\n","\t Val. Loss: 1.359 |  Val. Fscore: 72.41%\n","Epoch: 09 | Epoch Time: 0m 7s\n","\tTrain Loss: 0.025 | Train Fscore: 99.45%\n","\t Val. Loss: 1.455 |  Val. Fscore: 72.50%\n","Epoch: 10 | Epoch Time: 0m 7s\n","\tTrain Loss: 0.023 | Train Fscore: 99.53%\n","\t Val. Loss: 1.563 |  Val. Fscore: 72.87%\n","Epoch: 11 | Epoch Time: 0m 7s\n","\tTrain Loss: 0.018 | Train Fscore: 99.67%\n","\t Val. Loss: 1.643 |  Val. Fscore: 71.92%\n","Epoch: 12 | Epoch Time: 0m 7s\n","\tTrain Loss: 0.017 | Train Fscore: 99.65%\n","\t Val. Loss: 1.634 |  Val. Fscore: 71.80%\n","Epoch: 13 | Epoch Time: 0m 7s\n","\tTrain Loss: 0.016 | Train Fscore: 99.70%\n","\t Val. Loss: 1.776 |  Val. Fscore: 71.87%\n","Epoch: 14 | Epoch Time: 0m 7s\n","\tTrain Loss: 0.014 | Train Fscore: 99.75%\n","\t Val. Loss: 1.865 |  Val. Fscore: 72.34%\n","Epoch: 15 | Epoch Time: 0m 7s\n","\tTrain Loss: 0.015 | Train Fscore: 99.73%\n","\t Val. Loss: 1.924 |  Val. Fscore: 71.55%\n","Epoch: 16 | Epoch Time: 0m 7s\n","\tTrain Loss: 0.012 | Train Fscore: 99.71%\n","\t Val. Loss: 1.931 |  Val. Fscore: 71.62%\n","Epoch: 17 | Epoch Time: 0m 7s\n","\tTrain Loss: 0.015 | Train Fscore: 99.71%\n","\t Val. Loss: 1.953 |  Val. Fscore: 72.02%\n","Epoch: 18 | Epoch Time: 0m 7s\n","\tTrain Loss: 0.012 | Train Fscore: 99.72%\n","\t Val. Loss: 1.968 |  Val. Fscore: 71.59%\n","Epoch: 19 | Epoch Time: 0m 7s\n","\tTrain Loss: 0.012 | Train Fscore: 99.75%\n","\t Val. Loss: 1.933 |  Val. Fscore: 72.03%\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Nj_Kgy8ka2Rd"},"source":["# ================ EXTRA LEVEL ===================="]},{"cell_type":"code","metadata":{"id":"J9PzH8M_a2Rd"},"source":[""],"execution_count":null,"outputs":[]}]}